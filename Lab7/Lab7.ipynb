{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1:  Removal of bias in embedding (`Word2Vec`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = wv[\"king\"]\n",
    "\n",
    "print(king.shape)\n",
    "\n",
    "print(wv.most_similar(positive=[\"king\", \"queen\"], topn=10))\n",
    "\n",
    "print(wv.similarity(\"king\", \"queen\" ))\n",
    "\n",
    "print(wv.most_similar(positive=[\"man\", \"rule\"], negative=[\"woman\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "embedded_size = 8\n",
    "\n",
    "# Load imdb dataset and print a few samples to check.\n",
    "#\n",
    "# IMDB: sentence (x) -> positive/negative (y)\n",
    "#\n",
    "# “The food was really good” \t\t\t\t -> pos\n",
    "# “The chicken crossed the road because it was uncooked” -> neg\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# x_train has a size (training_size, ). Because the sentences have variable size,\n",
    "# we cannot represent this in matrix format.\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# The first step is to make the column size constant.\n",
    "#\n",
    "# We do that by \"padding\" the sentences. If the sentences are bigger, we clip them.\n",
    "# If they are smaller, we insert a \"NO_WORD\" token to the sentence.\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# Let's see the first sentence\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "# Input shape should be now (training_size, maxlen)\n",
    "\n",
    "# Let's use an embedding to try to help to estimate \n",
    "\n",
    "xi = Input(x_train.shape[1:])\n",
    "\n",
    "# Embedding input is (training_size, maxlen)\n",
    "# Embedding output is (training_size, maxlen, embedded_size)\n",
    "\n",
    "x = Embedding(max_features, embedded_size, input_length=maxlen)(xi)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=xi, outputs=x)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "        x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "p = model.predict(x_test)\n",
    "\n",
    "#\n",
    "# What's the current accuracy for this model?\n",
    "# \n",
    "# Try to add a preloaded embedded from Glove from this model, see the\n",
    "# suggestion in\n",
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# \n",
    "# Question: why does this model may help you get better accuracy?\n",
    "# \n",
    "# Try to change the maxlen or the embedded_size and plot a 3D graph with\n",
    "# embedded_size x maxlen x accuracy in a python jupyter notebook.\n",
    "#\n",
    "# Also try Conv1D + MaxPooling1D to improve results\n",
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Sentiment Analysis with `LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "embedded_size = 20\n",
    "\n",
    "# Load imdb dataset and print a few samples to check.\n",
    "#\n",
    "# IMDB: sentence (x) -> positive/negative (y)\n",
    "#\n",
    "# “The food was really good” \t\t\t\t -> pos\n",
    "# “The chicken crossed the road because it was uncooked” -> neg\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# x_train has a size (training_size, ). Because the sentences have variable size,\n",
    "# we cannot represent this in matrix format.\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# The first step is to make the column size constant.\n",
    "#\n",
    "# We do that by \"padding\" the sentences. If the sentences are bigger, we clip them.\n",
    "# If they are smaller, we insert a \"NO_WORD\" token to the sentence.\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# Let's see the first sentence\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "# Input shape should be now (training_size, maxlen)\n",
    "\n",
    "# Let's use an embedding to try to help to estimate \n",
    "\n",
    "xi = Input(x_train.shape[1:])\n",
    "\n",
    "# Embedding input is (training_size, maxlen)\n",
    "# Embedding output is (training_size, maxlen, embedded_size)\n",
    "\n",
    "x = Embedding(max_features, embedded_size, input_length=maxlen)(xi)\n",
    "\n",
    "# Using LSTM to classify sentence as positive or negative\n",
    "#\n",
    "# “The chicken crossed the road because it was uncooked”\n",
    "#\n",
    "# h0 -> The \t\t-> h1\n",
    "# h1 -> chicken \t-> h2\n",
    "# h2 -> crossed \t-> h3\n",
    "# h3 -> the \t\t-> h4\n",
    "# h4 -> road\t\t-> h5\n",
    "# h5 -> because\t\t-> h6\n",
    "# h6 -> it\t\t-> h7\n",
    "# h7 -> was\t\t-> h8\n",
    "# h8 -> uncooked\t-> h9\n",
    "# h9 -> pos\n",
    "\n",
    "# return_sequences: Boolean. Whether to return the last output in the output\n",
    "#     sequence, or the full sequence.\n",
    "\n",
    "# return_state: Boolean. Whether to return the last state in addition to the\n",
    "#     output. The returned elements of the states list are the hidden state\n",
    "#     and the cell state, respectively.\n",
    "\n",
    "#\n",
    "# What's the difference between return_sequences and return_state?\n",
    "#\n",
    "\n",
    "x = LSTM(32)(x)\n",
    "\n",
    "#\n",
    "# Try to get accuracy on validation set over 90%.\n",
    "#\n",
    "\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=xi, outputs=x)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "        x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "p = model.predict(x_test)\n",
    "\n",
    "#\n",
    "# What's the current accuracy for this model?\n",
    "# \n",
    "# Try to change the maxlen or the embedded_size and plot a 3D grpah with\n",
    "# embedded_size x maxlen x accuracy in a python jupyter notebook.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "path = get_file(\"nietzsche.txt\",\n",
    "        origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "\n",
    "text = open(path).read().lower()\n",
    "print(\"corpus length:\", len(text))\n",
    "\n",
    "chars = set(text)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 100\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print(\"nb sequences:\", len(sentences))\n",
    "\n",
    "print(\"Vectorization...\")\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: 2 stacked LSTM\n",
    "print(\"Build model...\")\n",
    "xi = Input((maxlen, len(chars)))\n",
    "x = GRU(256, return_sequences=True)(xi)\n",
    "x = Dropout(0.2)(x)\n",
    "x = GRU(256, return_sequences=False)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(len(chars))(x)\n",
    "x = Activation(\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=xi, outputs=x)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "adam = Adam(0.003)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam)\n",
    "\n",
    "\n",
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = (np.log(a + 1e-8) / temperature).astype(np.float64)\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    try:\n",
    "      sample_result = np.argmax(np.random.multinomial(1, a, 1))\n",
    "    except ValueError:\n",
    "      error = 1.0 - np.sum(a)\n",
    "      a[0] += error\n",
    "      sample_result = np.argmax(np.random.multinomial(1, a, 1))\n",
    "    return sample_result\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Iteration\", iteration)\n",
    "\n",
    "    model.fit(X, y, batch_size=4096, epochs=4)\n",
    "    model.save_weights(\"weights.hdf5\")\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print(\"----- diversity:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print(\"----- Generating with seed: '\" + sentence + \"'\")\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(200):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            # predict next char\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            # full sentence being generated\n",
    "            generated += next_char\n",
    "\n",
    "            # shift sentence\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # let's consider only one sentence\n",
    "            if next_char == \".\":\n",
    "              break\n",
    "        print()\n",
    "\n",
    "# Now you each group will perform the following tasks.\n",
    "#\n",
    "#   Part 1)\n",
    "#\n",
    "#   - Each group will pick up one set of data samples:\n",
    "#     * assembly code (machine code z80, x86, ...)\n",
    "#     * latex corpus\n",
    "#     * html pages\n",
    "#     * linux kernel source code (https://github.com/torvalds/linux)\n",
    "#     * patents\n",
    "#     * ...\n",
    "#   - Modify the model to be trained in the corpus you chose\n",
    "#   - Present the results\n",
    "#  \n",
    "#   Part 2)\n",
    "#\n",
    "#   - Pick up a book from Gutenberg (https://www.gutenberg.org/).\n",
    "#   - Extract tokens from the book. You will need to keep the Tokenizer map\n",
    "#     to generate the text\n",
    "#   - Use embeddings + glove as the first layer (https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "#   - Train a model to try to predict the next word of the book.\n",
    "#   - Be careful with starting-tokens and invalid-tokens.\n",
    "#   - Read a seed word.\n",
    "#   - Generate text based on the seed word.\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Translation with `seq2seq` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Implement seq2seq based on\n",
    "#\n",
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "# to translate from english to portuguese. Portuguese dictionary can be found\n",
    "# in http://www.manythings.org/anki/\n",
    "\n",
    "# 2. Change seq2seq to generate automated responses english to portuguese in\n",
    "# word-level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
