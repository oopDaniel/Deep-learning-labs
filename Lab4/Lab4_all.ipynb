{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tW5eOvGLlWkI"
   },
   "source": [
    "# CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNgS9qu1lhb7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.initializers import glorot_normal, RandomNormal\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_data(test_split=0.2, verbose=True):\n",
    "    \"\"\"load the raw data\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        test_split {float} -- [description] (default: {0.2})\n",
    "        verbose {bool} -- output dataset details (default: {True})\n",
    "\n",
    "    return:\n",
    "        (x_train, y_train, x_test, y_test)\n",
    "    \"\"\"    \n",
    "    # load raw data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "def preprocessing(x_train, y_train, x_test, y_test, verbose=True):\n",
    "    # Normalization\n",
    "    mean = np.mean(x_train, axis=(0,1,2,3))\n",
    "    std = np.std(x_train, axis=(0,1,2,3))\n",
    "    x_train = (x_train - mean) / (std+1e-7)\n",
    "    x_test = (x_test - mean) / (std+1e-7)\n",
    "\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Shape of x_train: {x_train.shape}, Shape of y_train: {y_train.shape}\\n\" +\n",
    "            f\"Shape of x_test:  {x_test.shape }, Shape of y_test:  {y_test.shape}\\n\"\n",
    "        )\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def get_model(x_train, strides=2, weight_decay = 1e-2, act=\"relu\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_initializer=glorot_normal(), input_shape=x_train.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 4\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    # First Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=strides))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 5\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 6\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 7\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    # Second Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=strides))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 8\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 9\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=strides))\n",
    "    \n",
    "    \n",
    "    # Block 10\n",
    "    model.add(Conv2D(512, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 11  \n",
    "    model.add(Conv2D(2048, (1,1), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 12  \n",
    "    model.add(Conv2D(256, (1,1), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fourth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=strides))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    # Block 13\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fifth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=strides))\n",
    "\n",
    "    # Final Classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt_adm = Adadelta(lr=0.1) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(model, x_train, y_train, x_test, y_test, optimizer, img_gen, epochs, batch=128):\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=optimizer, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        img_gen.flow(x_train, y_train, batch_size=batch),\n",
    "        steps_per_epoch = x_train.shape[0] // batch,\n",
    "        validation_data = (x_test,y_test), \n",
    "        epochs = epochs\n",
    "    )\n",
    "\n",
    "    return (model, history)\n",
    "\n",
    "\n",
    "def plot(history, title):\n",
    "    fig, ax = plt.subplots(, 1, figsize=(7, 7), sharex=True)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    ax[0].plot(\n",
    "        history['loss'], \"red\", \n",
    "        label=f\"loss = {round(history['loss'][-1], 4)}\"\n",
    "    )\n",
    "    ax[0].set_title(\"loss\")\n",
    "    \n",
    "    accuracy = str(round(history['accuracy'][-1], 4))\n",
    "    ax[1].plot(\n",
    "        history['accuracy'], \"blue\", \n",
    "        label=f\"accuracy = {accuracy}\"\n",
    "    )\n",
    "    ax[1].set_title(\"accuracy\")\n",
    "    ax[1].set_xlabel(\"Iterations\")\n",
    "    fig.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3142858,
     "status": "ok",
     "timestamp": 1581018784290,
     "user": {
      "displayName": "Mingyang Zhao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBU_4wI4WvnINy6zSsai9F1QxIA_IipMaofL5MFhDOTeReb5ylbXw_ivpZDOI5wsqk98RaR0qRyDCUUfFSzhGgIoLmxFA3QaMTUnQmV2fdeQBHBrhw-TdI1KzVrHNVKHHnRzlJgdzEhX2VdgVfr1-swZoVvqC4I7mqiacBbscwI6t4kZKm7sgBHxVcqdsciEmYRvktvMX4ukT1WM9ixLs8_gj0ECnkcK0uTM6Vj8R6f7b-xupkFz6pkEaj3V1I07ma5u-j_hcwdb0b-9BGbiMFJMnbyKWpYWwlhUgze94OFW3MVlRo6DzlqzqNep58KPMUoof2hWUrPZEPfSCUhIX47CQ3vSXeCNtjzQf9D1gZV7LfAfq7wjp76UE38-8sfKdb4Bk0_vjjjaSPb8M9iw2jxpDnSzOKBTlztXA91x35R6ZkQo0Yxsfvr7F5e0lFnza7WJm8PzO-hE0kTFpHTwUCwwE3RVNGJd2pAzQzrHx4GtDB7Y74Iry6xo575jdrYgbO7pLBToS1wnywpK5KKT0nOxXTcPhBiUjzKEyFeVXVyYWYpyGpdbHbETi5STP59GeSpPXBnjgxNIQ2tYYkeRSnJMPE3JeygS3wvLjVcqVwZu809cnKmM1pC2M7TzuV2n1BX_9SAf64GVBRtyNtk35H7uwyZB1hrcggRT3na3OkXdIwJXjWbVOIOI9Q9MZRdidDFx9S-EF7i2xByOZeW9vT30s0K9yl2Tui328a989Nk8ycbDgo_GOq4cTHQqzQ=s64",
      "userId": "06904678479789034552"
     },
     "user_tz": 480
    },
    "id": "hvYTdhI1z_O1",
    "outputId": "9f4dff30-c1c3-4ecd-c982-6feb21a8e7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n",
      "Dataset summary\n",
      "-------------------------------\n",
      "Shape of x_train: (50000, 32, 32, 3), Shape of y_train: (50000, 10)\n",
      "Shape of x_test:  (10000, 32, 32, 3), Shape of y_test:  (10000, 10)\n",
      "\n",
      "Model Summary\n",
      "-------------------------------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 256)         524544    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 5,497,226\n",
      "Trainable params: 5,493,258\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 390 steps, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "390/390 [==============================] - 52s 133ms/step - loss: 1.7801 - accuracy: 0.3293 - val_loss: 3.5040 - val_accuracy: 0.1141\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 1.4174 - accuracy: 0.4750 - val_loss: 1.7812 - val_accuracy: 0.3758\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 1.2037 - accuracy: 0.5620 - val_loss: 1.6142 - val_accuracy: 0.4582\n",
      "Epoch 4/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 1.0475 - accuracy: 0.6225 - val_loss: 1.1933 - val_accuracy: 0.5790\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.9275 - accuracy: 0.6686 - val_loss: 1.1313 - val_accuracy: 0.5992\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.8456 - accuracy: 0.6998 - val_loss: 0.9581 - val_accuracy: 0.6657\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.7806 - accuracy: 0.7258 - val_loss: 0.7968 - val_accuracy: 0.7186\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.7186 - accuracy: 0.7469 - val_loss: 0.9374 - val_accuracy: 0.6784\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.6796 - accuracy: 0.7623 - val_loss: 0.9102 - val_accuracy: 0.6867\n",
      "Epoch 10/50\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.6476 - accuracy: 0.7750 - val_loss: 0.7322 - val_accuracy: 0.7459\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.6122 - accuracy: 0.7868 - val_loss: 0.7103 - val_accuracy: 0.7533\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.5870 - accuracy: 0.7950 - val_loss: 0.5624 - val_accuracy: 0.8058\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.5587 - accuracy: 0.8062 - val_loss: 0.7346 - val_accuracy: 0.7583\n",
      "Epoch 14/50\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.5383 - accuracy: 0.8132 - val_loss: 0.5282 - val_accuracy: 0.8173\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.5206 - accuracy: 0.8185 - val_loss: 0.6283 - val_accuracy: 0.7809\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.5004 - accuracy: 0.8256 - val_loss: 0.5746 - val_accuracy: 0.8037\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.4846 - accuracy: 0.8320 - val_loss: 0.6628 - val_accuracy: 0.7736\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.4669 - accuracy: 0.8369 - val_loss: 0.4426 - val_accuracy: 0.8475\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.4611 - accuracy: 0.8396 - val_loss: 0.5300 - val_accuracy: 0.8170\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.4398 - accuracy: 0.8474 - val_loss: 0.5339 - val_accuracy: 0.8156\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.4353 - accuracy: 0.8503 - val_loss: 0.4426 - val_accuracy: 0.8530\n",
      "Epoch 22/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.4212 - accuracy: 0.8537 - val_loss: 0.4948 - val_accuracy: 0.8343\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.4115 - accuracy: 0.8579 - val_loss: 0.4476 - val_accuracy: 0.8494\n",
      "Epoch 24/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.3960 - accuracy: 0.8617 - val_loss: 0.4871 - val_accuracy: 0.8370\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3850 - accuracy: 0.8659 - val_loss: 0.4549 - val_accuracy: 0.8433\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.3789 - accuracy: 0.8678 - val_loss: 0.5025 - val_accuracy: 0.8314\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3731 - accuracy: 0.8690 - val_loss: 0.4323 - val_accuracy: 0.8563\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3612 - accuracy: 0.8732 - val_loss: 0.3925 - val_accuracy: 0.8675\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3611 - accuracy: 0.8732 - val_loss: 0.4276 - val_accuracy: 0.8633\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3467 - accuracy: 0.8782 - val_loss: 0.4327 - val_accuracy: 0.8551\n",
      "Epoch 31/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3407 - accuracy: 0.8810 - val_loss: 0.4196 - val_accuracy: 0.8607\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - 41s 106ms/step - loss: 0.3345 - accuracy: 0.8830 - val_loss: 0.4365 - val_accuracy: 0.8502\n",
      "Epoch 33/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3251 - accuracy: 0.8868 - val_loss: 0.4234 - val_accuracy: 0.8578\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3224 - accuracy: 0.8889 - val_loss: 0.4053 - val_accuracy: 0.8668\n",
      "Epoch 35/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3144 - accuracy: 0.8890 - val_loss: 0.3586 - val_accuracy: 0.8817\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3107 - accuracy: 0.8917 - val_loss: 0.3726 - val_accuracy: 0.8760\n",
      "Epoch 37/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3036 - accuracy: 0.8941 - val_loss: 0.4254 - val_accuracy: 0.8604\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.3002 - accuracy: 0.8945 - val_loss: 0.3425 - val_accuracy: 0.8869\n",
      "Epoch 39/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.2904 - accuracy: 0.8975 - val_loss: 0.3692 - val_accuracy: 0.8752\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.2808 - accuracy: 0.9012 - val_loss: 0.3557 - val_accuracy: 0.8808\n",
      "Epoch 41/50\n",
      "390/390 [==============================] - 42s 106ms/step - loss: 0.2799 - accuracy: 0.9023 - val_loss: 0.3651 - val_accuracy: 0.8785\n",
      "Epoch 42/50\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.2772 - accuracy: 0.9021 - val_loss: 0.3977 - val_accuracy: 0.8658\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - 41s 106ms/step - loss: 0.2708 - accuracy: 0.9057 - val_loss: 0.3665 - val_accuracy: 0.8782\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.2665 - accuracy: 0.9061 - val_loss: 0.3345 - val_accuracy: 0.8891\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.2636 - accuracy: 0.9073 - val_loss: 0.3420 - val_accuracy: 0.8880\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.2567 - accuracy: 0.9093 - val_loss: 0.3466 - val_accuracy: 0.8855\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.2531 - accuracy: 0.9123 - val_loss: 0.3470 - val_accuracy: 0.8867\n",
      "Epoch 48/50\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.2482 - accuracy: 0.9124 - val_loss: 0.3468 - val_accuracy: 0.8883\n",
      "Epoch 49/50\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.2521 - accuracy: 0.9109 - val_loss: 0.3143 - val_accuracy: 0.8927\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.2448 - accuracy: 0.9126 - val_loss: 0.3566 - val_accuracy: 0.8842\n",
      "10000/10000 [==============================] - 3s 298us/sample - loss: 0.3566 - accuracy: 0.8842\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 390 steps, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.2160 - accuracy: 0.9251 - val_loss: 0.2859 - val_accuracy: 0.9057\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 45s 115ms/step - loss: 0.2040 - accuracy: 0.9283 - val_loss: 0.2779 - val_accuracy: 0.9086\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1993 - accuracy: 0.9307 - val_loss: 0.2887 - val_accuracy: 0.9059\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1972 - accuracy: 0.9302 - val_loss: 0.2912 - val_accuracy: 0.9046\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1935 - accuracy: 0.9328 - val_loss: 0.2824 - val_accuracy: 0.9081\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1922 - accuracy: 0.9321 - val_loss: 0.2963 - val_accuracy: 0.9041\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1911 - accuracy: 0.9328 - val_loss: 0.2848 - val_accuracy: 0.9067\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1906 - accuracy: 0.9320 - val_loss: 0.2796 - val_accuracy: 0.9089\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 43s 111ms/step - loss: 0.1886 - accuracy: 0.9334 - val_loss: 0.2776 - val_accuracy: 0.9100\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1870 - accuracy: 0.9352 - val_loss: 0.2792 - val_accuracy: 0.9086\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1853 - accuracy: 0.9349 - val_loss: 0.2786 - val_accuracy: 0.9092\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1829 - accuracy: 0.9350 - val_loss: 0.2792 - val_accuracy: 0.9089\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1854 - accuracy: 0.9343 - val_loss: 0.2815 - val_accuracy: 0.9096\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1817 - accuracy: 0.9369 - val_loss: 0.2815 - val_accuracy: 0.9071\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1851 - accuracy: 0.9345 - val_loss: 0.2818 - val_accuracy: 0.9081\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1832 - accuracy: 0.9353 - val_loss: 0.2707 - val_accuracy: 0.9099\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 43s 111ms/step - loss: 0.1794 - accuracy: 0.9362 - val_loss: 0.2741 - val_accuracy: 0.9096\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1792 - accuracy: 0.9355 - val_loss: 0.2710 - val_accuracy: 0.9136\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1822 - accuracy: 0.9353 - val_loss: 0.2846 - val_accuracy: 0.9063\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1812 - accuracy: 0.9361 - val_loss: 0.2752 - val_accuracy: 0.9105\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1787 - accuracy: 0.9357 - val_loss: 0.2691 - val_accuracy: 0.9119\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1743 - accuracy: 0.9383 - val_loss: 0.2677 - val_accuracy: 0.9115\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1761 - accuracy: 0.9383 - val_loss: 0.2682 - val_accuracy: 0.9112\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1765 - accuracy: 0.9369 - val_loss: 0.2725 - val_accuracy: 0.9105\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 43s 111ms/step - loss: 0.1746 - accuracy: 0.9383 - val_loss: 0.2668 - val_accuracy: 0.9121\n",
      "10000/10000 [==============================] - 3s 300us/sample - loss: 0.2668 - accuracy: 0.9121\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 390 steps, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "390/390 [==============================] - 45s 115ms/step - loss: 0.1740 - accuracy: 0.9395 - val_loss: 0.2727 - val_accuracy: 0.9112\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1700 - accuracy: 0.9415 - val_loss: 0.2740 - val_accuracy: 0.9107\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1696 - accuracy: 0.9403 - val_loss: 0.2722 - val_accuracy: 0.9111\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1733 - accuracy: 0.9379 - val_loss: 0.2729 - val_accuracy: 0.9107\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1688 - accuracy: 0.9400 - val_loss: 0.2721 - val_accuracy: 0.9114\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1715 - accuracy: 0.9387 - val_loss: 0.2708 - val_accuracy: 0.9120\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1682 - accuracy: 0.9407 - val_loss: 0.2726 - val_accuracy: 0.9117\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1695 - accuracy: 0.9397 - val_loss: 0.2731 - val_accuracy: 0.9103\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.1654 - accuracy: 0.9406 - val_loss: 0.2721 - val_accuracy: 0.9116\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1726 - accuracy: 0.9380 - val_loss: 0.2725 - val_accuracy: 0.9116\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1679 - accuracy: 0.9401 - val_loss: 0.2726 - val_accuracy: 0.9115\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1689 - accuracy: 0.9398 - val_loss: 0.2731 - val_accuracy: 0.9112\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1693 - accuracy: 0.9409 - val_loss: 0.2728 - val_accuracy: 0.9111\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1684 - accuracy: 0.9401 - val_loss: 0.2738 - val_accuracy: 0.9107\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1659 - accuracy: 0.9409 - val_loss: 0.2731 - val_accuracy: 0.9103\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1664 - accuracy: 0.9418 - val_loss: 0.2729 - val_accuracy: 0.9106\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1665 - accuracy: 0.9416 - val_loss: 0.2719 - val_accuracy: 0.9111\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1669 - accuracy: 0.9411 - val_loss: 0.2710 - val_accuracy: 0.9119\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1661 - accuracy: 0.9404 - val_loss: 0.2734 - val_accuracy: 0.9112\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1659 - accuracy: 0.9408 - val_loss: 0.2720 - val_accuracy: 0.9117\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1686 - accuracy: 0.9404 - val_loss: 0.2719 - val_accuracy: 0.9119\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1685 - accuracy: 0.9408 - val_loss: 0.2696 - val_accuracy: 0.9126\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1675 - accuracy: 0.9403 - val_loss: 0.2724 - val_accuracy: 0.9110\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1670 - accuracy: 0.9411 - val_loss: 0.2710 - val_accuracy: 0.9118\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1668 - accuracy: 0.9413 - val_loss: 0.2712 - val_accuracy: 0.9119\n",
      "10000/10000 [==============================] - 3s 298us/sample - loss: 0.2712 - accuracy: 0.9119\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 390 steps, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "390/390 [==============================] - 45s 115ms/step - loss: 0.1673 - accuracy: 0.9407 - val_loss: 0.2704 - val_accuracy: 0.9118\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1705 - accuracy: 0.9385 - val_loss: 0.2708 - val_accuracy: 0.9117\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1677 - accuracy: 0.9412 - val_loss: 0.2706 - val_accuracy: 0.9118\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1706 - accuracy: 0.9395 - val_loss: 0.2706 - val_accuracy: 0.9116\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1691 - accuracy: 0.9404 - val_loss: 0.2721 - val_accuracy: 0.9112\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1699 - accuracy: 0.9406 - val_loss: 0.2715 - val_accuracy: 0.9113\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 42s 107ms/step - loss: 0.1615 - accuracy: 0.9423 - val_loss: 0.2710 - val_accuracy: 0.9113\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1648 - accuracy: 0.9421 - val_loss: 0.2706 - val_accuracy: 0.9119\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1685 - accuracy: 0.9404 - val_loss: 0.2713 - val_accuracy: 0.9115\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 0.1694 - accuracy: 0.9397 - val_loss: 0.2717 - val_accuracy: 0.9112\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1661 - accuracy: 0.9413 - val_loss: 0.2708 - val_accuracy: 0.9113\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1689 - accuracy: 0.9405 - val_loss: 0.2712 - val_accuracy: 0.9113\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 42s 109ms/step - loss: 0.1675 - accuracy: 0.9405 - val_loss: 0.2707 - val_accuracy: 0.9118\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1664 - accuracy: 0.9416 - val_loss: 0.2712 - val_accuracy: 0.9115\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1698 - accuracy: 0.9390 - val_loss: 0.2713 - val_accuracy: 0.9113\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1699 - accuracy: 0.9404 - val_loss: 0.2704 - val_accuracy: 0.9118\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1672 - accuracy: 0.9396 - val_loss: 0.2703 - val_accuracy: 0.9116\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1684 - accuracy: 0.9395 - val_loss: 0.2708 - val_accuracy: 0.9116\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1657 - accuracy: 0.9421 - val_loss: 0.2710 - val_accuracy: 0.9115\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1697 - accuracy: 0.9406 - val_loss: 0.2718 - val_accuracy: 0.9112\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1711 - accuracy: 0.9398 - val_loss: 0.2707 - val_accuracy: 0.9115\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1665 - accuracy: 0.9407 - val_loss: 0.2706 - val_accuracy: 0.9113\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 43s 110ms/step - loss: 0.1658 - accuracy: 0.9418 - val_loss: 0.2714 - val_accuracy: 0.9112\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 43s 109ms/step - loss: 0.1677 - accuracy: 0.9404 - val_loss: 0.2708 - val_accuracy: 0.9113\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 43s 111ms/step - loss: 0.1664 - accuracy: 0.9410 - val_loss: 0.2715 - val_accuracy: 0.9112\n",
      "10000/10000 [==============================] - 3s 299us/sample - loss: 0.2715 - accuracy: 0.9112\n"
     ]
    }
   ],
   "source": [
    "def main(steps=4):\n",
    "    # Load and preprocessing data\n",
    "    x_train, y_train, x_test, y_test = preprocessing(*load_data())\n",
    "\n",
    "    # Data augmentation\n",
    "    img_gen = ImageDataGenerator(\n",
    "        width_shift_range  = 0.1,\n",
    "        height_shift_range = 0.1,\n",
    "        rotation_range     = 15,\n",
    "        horizontal_flip    = True\n",
    "    )\n",
    "    img_gen.fit(x_train)\n",
    "    \n",
    "    # Create model\n",
    "\n",
    "    model = get_model(x_train)\n",
    "    model.summary()\n",
    "\n",
    "    # fit model and record history\n",
    "    history = {}\n",
    "    for step in range(1, steps + 1):\n",
    "\n",
    "        learning_rate = 0.1 ** step\n",
    "        optimizer = Adadelta(learning_rate)\n",
    "\n",
    "        epochs = 50 if step == 1 else 25\n",
    "\n",
    "        model, history[step] = fit_model(\n",
    "            model, x_train, y_train, x_test, y_test, \n",
    "            optimizer = optimizer, \n",
    "            img_gen   = img_gen, \n",
    "            epochs    = epochs\n",
    "        )\n",
    "\n",
    "        model.evaluate(x_test, y_test)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_c10, history_c10 = main(steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5851,
     "status": "ok",
     "timestamp": 1581020109536,
     "user": {
      "displayName": "Mingyang Zhao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBU_4wI4WvnINy6zSsai9F1QxIA_IipMaofL5MFhDOTeReb5ylbXw_ivpZDOI5wsqk98RaR0qRyDCUUfFSzhGgIoLmxFA3QaMTUnQmV2fdeQBHBrhw-TdI1KzVrHNVKHHnRzlJgdzEhX2VdgVfr1-swZoVvqC4I7mqiacBbscwI6t4kZKm7sgBHxVcqdsciEmYRvktvMX4ukT1WM9ixLs8_gj0ECnkcK0uTM6Vj8R6f7b-xupkFz6pkEaj3V1I07ma5u-j_hcwdb0b-9BGbiMFJMnbyKWpYWwlhUgze94OFW3MVlRo6DzlqzqNep58KPMUoof2hWUrPZEPfSCUhIX47CQ3vSXeCNtjzQf9D1gZV7LfAfq7wjp76UE38-8sfKdb4Bk0_vjjjaSPb8M9iw2jxpDnSzOKBTlztXA91x35R6ZkQo0Yxsfvr7F5e0lFnza7WJm8PzO-hE0kTFpHTwUCwwE3RVNGJd2pAzQzrHx4GtDB7Y74Iry6xo575jdrYgbO7pLBToS1wnywpK5KKT0nOxXTcPhBiUjzKEyFeVXVyYWYpyGpdbHbETi5STP59GeSpPXBnjgxNIQ2tYYkeRSnJMPE3JeygS3wvLjVcqVwZu809cnKmM1pC2M7TzuV2n1BX_9SAf64GVBRtyNtk35H7uwyZB1hrcggRT3na3OkXdIwJXjWbVOIOI9Q9MZRdidDFx9S-EF7i2xByOZeW9vT30s0K9yl2Tui328a989Nk8ycbDgo_GOq4cTHQqzQ=s64",
      "userId": "06904678479789034552"
     },
     "user_tz": 480
    },
    "id": "ACZ1loVCFrA9",
    "outputId": "91a2a5ac-b828-475e-bf63-24af3e5742ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset summary\n",
      "-------------------------------\n",
      "Shape of x_train: (50000, 32, 32, 3), Shape of y_train: (50000, 10)\n",
      "Shape of x_test:  (10000, 32, 32, 3), Shape of y_test:  (10000, 10)\n",
      "\n",
      "Test set accuracy\n",
      "-------------------------------\n",
      "10000/10000 [==============================] - 3s 290us/sample - loss: 0.2715 - accuracy: 0.9112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27146184172034266, 0.9112]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, x_test, y_test = preprocessing(*load_data())\n",
    "\n",
    "print(\"\"\"\\\n",
    "Test set accuracy\n",
    "-------------------------------\"\"\")\n",
    "model_c10.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1187,
     "status": "ok",
     "timestamp": 1581021399448,
     "user": {
      "displayName": "Mingyang Zhao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBU_4wI4WvnINy6zSsai9F1QxIA_IipMaofL5MFhDOTeReb5ylbXw_ivpZDOI5wsqk98RaR0qRyDCUUfFSzhGgIoLmxFA3QaMTUnQmV2fdeQBHBrhw-TdI1KzVrHNVKHHnRzlJgdzEhX2VdgVfr1-swZoVvqC4I7mqiacBbscwI6t4kZKm7sgBHxVcqdsciEmYRvktvMX4ukT1WM9ixLs8_gj0ECnkcK0uTM6Vj8R6f7b-xupkFz6pkEaj3V1I07ma5u-j_hcwdb0b-9BGbiMFJMnbyKWpYWwlhUgze94OFW3MVlRo6DzlqzqNep58KPMUoof2hWUrPZEPfSCUhIX47CQ3vSXeCNtjzQf9D1gZV7LfAfq7wjp76UE38-8sfKdb4Bk0_vjjjaSPb8M9iw2jxpDnSzOKBTlztXA91x35R6ZkQo0Yxsfvr7F5e0lFnza7WJm8PzO-hE0kTFpHTwUCwwE3RVNGJd2pAzQzrHx4GtDB7Y74Iry6xo575jdrYgbO7pLBToS1wnywpK5KKT0nOxXTcPhBiUjzKEyFeVXVyYWYpyGpdbHbETi5STP59GeSpPXBnjgxNIQ2tYYkeRSnJMPE3JeygS3wvLjVcqVwZu809cnKmM1pC2M7TzuV2n1BX_9SAf64GVBRtyNtk35H7uwyZB1hrcggRT3na3OkXdIwJXjWbVOIOI9Q9MZRdidDFx9S-EF7i2xByOZeW9vT30s0K9yl2Tui328a989Nk8ycbDgo_GOq4cTHQqzQ=s64",
      "userId": "06904678479789034552"
     },
     "user_tz": 480
    },
    "id": "z6jmPrpoFtzf",
    "outputId": "741ad64b-2fb8-4d35-fe29-5f0dacf64aaf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAHhCAYAAAAvcLSFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_all = {}\n",
    "\n",
    "for h in history_c10:\n",
    "    h_dict = history_c10[h].history\n",
    "    for k in h_dict:\n",
    "        history_all[k] = history_all.get(k, []) + h_dict[k]\n",
    "\n",
    "plot(history_all, \"CIFAR 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gLao_EkJlA0"
   },
   "source": [
    "# CIFAR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9919,
     "status": "ok",
     "timestamp": 1581037578816,
     "user": {
      "displayName": "Mingyang Zhao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBU_4wI4WvnINy6zSsai9F1QxIA_IipMaofL5MFhDOTeReb5ylbXw_ivpZDOI5wsqk98RaR0qRyDCUUfFSzhGgIoLmxFA3QaMTUnQmV2fdeQBHBrhw-TdI1KzVrHNVKHHnRzlJgdzEhX2VdgVfr1-swZoVvqC4I7mqiacBbscwI6t4kZKm7sgBHxVcqdsciEmYRvktvMX4ukT1WM9ixLs8_gj0ECnkcK0uTM6Vj8R6f7b-xupkFz6pkEaj3V1I07ma5u-j_hcwdb0b-9BGbiMFJMnbyKWpYWwlhUgze94OFW3MVlRo6DzlqzqNep58KPMUoof2hWUrPZEPfSCUhIX47CQ3vSXeCNtjzQf9D1gZV7LfAfq7wjp76UE38-8sfKdb4Bk0_vjjjaSPb8M9iw2jxpDnSzOKBTlztXA91x35R6ZkQo0Yxsfvr7F5e0lFnza7WJm8PzO-hE0kTFpHTwUCwwE3RVNGJd2pAzQzrHx4GtDB7Y74Iry6xo575jdrYgbO7pLBToS1wnywpK5KKT0nOxXTcPhBiUjzKEyFeVXVyYWYpyGpdbHbETi5STP59GeSpPXBnjgxNIQ2tYYkeRSnJMPE3JeygS3wvLjVcqVwZu809cnKmM1pC2M7TzuV2n1BX_9SAf64GVBRtyNtk35H7uwyZB1hrcggRT3na3OkXdIwJXjWbVOIOI9Q9MZRdidDFx9S-EF7i2xByOZeW9vT30s0K9yl2Tui328a989Nk8ycbDgo_GOq4cTHQqzQ=s64",
      "userId": "06904678479789034552"
     },
     "user_tz": 480
    },
    "id": "x-k8Ro5dJmCo",
    "outputId": "5fa042fd-c746-4a33-f860-7d47e3bff13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def load_data(test_split=0.2, verbose=True):\n",
    "    \"\"\"load the raw data\n",
    "    \n",
    "    Keyword Arguments:\n",
    "        test_split {float} -- [description] (default: {0.2})\n",
    "        verbose {bool} -- output dataset details (default: {True})\n",
    "\n",
    "    return:\n",
    "        (x_train, y_train, x_test, y_test)\n",
    "    \"\"\"    \n",
    "    # load raw data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "def preprocessing(x_train, y_train, x_test, y_test, verbose=True):\n",
    "    # Normalization\n",
    "    mean = np.mean(x_train, axis=(0,1,2,3))\n",
    "    std  = np.std( x_train, axis=(0,1,2,3))\n",
    "    x_train = (x_train - mean)/(std+1e-7)\n",
    "    x_test  = (x_test  - mean)/(std+1e-7)\n",
    "\n",
    "    y_train = to_categorical(y_train, 100)\n",
    "    y_test  = to_categorical(y_test,  100)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Shape of x_train: {x_train.shape}, Shape of y_train: {y_train.shape}\\n\" +\n",
    "            f\"Shape of x_test:  {x_test.shape }, Shape of y_test:  {y_test.shape}\\n\"\n",
    "        )\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def plot(history, title):\n",
    "    plt.title(title)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.plot(\n",
    "        history['loss'], \n",
    "        label=f\"loss = {history['loss'][-1]}\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1634715,
     "status": "ok",
     "timestamp": 1581039208311,
     "user": {
      "displayName": "Mingyang Zhao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBU_4wI4WvnINy6zSsai9F1QxIA_IipMaofL5MFhDOTeReb5ylbXw_ivpZDOI5wsqk98RaR0qRyDCUUfFSzhGgIoLmxFA3QaMTUnQmV2fdeQBHBrhw-TdI1KzVrHNVKHHnRzlJgdzEhX2VdgVfr1-swZoVvqC4I7mqiacBbscwI6t4kZKm7sgBHxVcqdsciEmYRvktvMX4ukT1WM9ixLs8_gj0ECnkcK0uTM6Vj8R6f7b-xupkFz6pkEaj3V1I07ma5u-j_hcwdb0b-9BGbiMFJMnbyKWpYWwlhUgze94OFW3MVlRo6DzlqzqNep58KPMUoof2hWUrPZEPfSCUhIX47CQ3vSXeCNtjzQf9D1gZV7LfAfq7wjp76UE38-8sfKdb4Bk0_vjjjaSPb8M9iw2jxpDnSzOKBTlztXA91x35R6ZkQo0Yxsfvr7F5e0lFnza7WJm8PzO-hE0kTFpHTwUCwwE3RVNGJd2pAzQzrHx4GtDB7Y74Iry6xo575jdrYgbO7pLBToS1wnywpK5KKT0nOxXTcPhBiUjzKEyFeVXVyYWYpyGpdbHbETi5STP59GeSpPXBnjgxNIQ2tYYkeRSnJMPE3JeygS3wvLjVcqVwZu809cnKmM1pC2M7TzuV2n1BX_9SAf64GVBRtyNtk35H7uwyZB1hrcggRT3na3OkXdIwJXjWbVOIOI9Q9MZRdidDFx9S-EF7i2xByOZeW9vT30s0K9yl2Tui328a989Nk8ycbDgo_GOq4cTHQqzQ=s64",
      "userId": "06904678479789034552"
     },
     "user_tz": 480
    },
    "id": "-RYUYnYfJplO",
    "outputId": "7707fb34-30da-47e2-feb0-e7d6213dd1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 6s 0us/step\n",
      "Dataset summary\n",
      "-------------------------------\n",
      "Shape of x_train: (50000, 32, 32, 3), Shape of y_train: (50000, 100)\n",
      "Shape of x_test:  (10000, 32, 32, 3), Shape of y_test:  (10000, 100)\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 564,484\n",
      "Trainable params: 563,332\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 16s 329us/sample - loss: 4.7998 - accuracy: 0.0372\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 4.1605 - accuracy: 0.0804\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 3.9012 - accuracy: 0.1096\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 3.7240 - accuracy: 0.1327\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 3.5751 - accuracy: 0.1541\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 3.4560 - accuracy: 0.1749\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 3.3593 - accuracy: 0.1917\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 3.2679 - accuracy: 0.2035\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 3.1864 - accuracy: 0.2200\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 3.1079 - accuracy: 0.2311\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 3.0277 - accuracy: 0.2479\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.9687 - accuracy: 0.2600\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.9160 - accuracy: 0.2680\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 2.8608 - accuracy: 0.2791\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 8s 164us/sample - loss: 2.8098 - accuracy: 0.2871\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.7694 - accuracy: 0.2965\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 2.7207 - accuracy: 0.3087\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 2.6921 - accuracy: 0.3144\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.6506 - accuracy: 0.3200\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.6182 - accuracy: 0.3289\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.5821 - accuracy: 0.3383\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.5549 - accuracy: 0.3427\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.5230 - accuracy: 0.3450\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.4961 - accuracy: 0.3524\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.4694 - accuracy: 0.3602\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 2.4389 - accuracy: 0.3638\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.4255 - accuracy: 0.3689\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.3982 - accuracy: 0.3754\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 2.3777 - accuracy: 0.3787\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 8s 165us/sample - loss: 2.3587 - accuracy: 0.3822\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.3335 - accuracy: 0.3873\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 2.3128 - accuracy: 0.3900\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 2.2996 - accuracy: 0.3962\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.2739 - accuracy: 0.3991\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 8s 165us/sample - loss: 2.2573 - accuracy: 0.4029\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 8s 165us/sample - loss: 2.2307 - accuracy: 0.4102\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 2.2217 - accuracy: 0.4090\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.2052 - accuracy: 0.4158\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 2.1947 - accuracy: 0.4191\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 2.1748 - accuracy: 0.4220\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.1588 - accuracy: 0.4260\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 2.1514 - accuracy: 0.4275\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.1260 - accuracy: 0.4319\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 2.1126 - accuracy: 0.4337\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.1052 - accuracy: 0.4388\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.0890 - accuracy: 0.4413\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 2.0691 - accuracy: 0.4433\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.0594 - accuracy: 0.4474\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 2.0412 - accuracy: 0.4505\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 2.0347 - accuracy: 0.4492\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 2.0355 - accuracy: 0.4515\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 8s 164us/sample - loss: 2.0160 - accuracy: 0.4568\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.9980 - accuracy: 0.4584\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.9864 - accuracy: 0.4645\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.9734 - accuracy: 0.4664\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.9594 - accuracy: 0.4708\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.9542 - accuracy: 0.4699\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.9439 - accuracy: 0.4758\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.9230 - accuracy: 0.4751\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.9238 - accuracy: 0.4766\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.9170 - accuracy: 0.4793\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.8963 - accuracy: 0.4834\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.8943 - accuracy: 0.4824\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.8764 - accuracy: 0.4875\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.8706 - accuracy: 0.4905\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.8590 - accuracy: 0.4893\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 8s 166us/sample - loss: 1.8431 - accuracy: 0.4946\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.8365 - accuracy: 0.4958\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.8315 - accuracy: 0.4992\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.8300 - accuracy: 0.4993\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.8111 - accuracy: 0.5019\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.8107 - accuracy: 0.5036\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.8001 - accuracy: 0.5047\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 8s 166us/sample - loss: 1.7830 - accuracy: 0.5091\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.7773 - accuracy: 0.5058\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.7758 - accuracy: 0.5101\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.7632 - accuracy: 0.5121\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.7500 - accuracy: 0.5164\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.7454 - accuracy: 0.5169\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.7304 - accuracy: 0.5201\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.7435 - accuracy: 0.5161\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 8s 164us/sample - loss: 1.7153 - accuracy: 0.5220\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.7241 - accuracy: 0.5234\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.7094 - accuracy: 0.5224\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.7024 - accuracy: 0.5271\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.6860 - accuracy: 0.5275\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.6811 - accuracy: 0.5311\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.6785 - accuracy: 0.5330\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.6706 - accuracy: 0.5296\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.6698 - accuracy: 0.5344\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.6576 - accuracy: 0.5355\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.6477 - accuracy: 0.5370\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.6324 - accuracy: 0.5442\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.6351 - accuracy: 0.5414\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.6313 - accuracy: 0.5436\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.6242 - accuracy: 0.5426\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.6194 - accuracy: 0.5436\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.6133 - accuracy: 0.5475\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5991 - accuracy: 0.5497\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.5991 - accuracy: 0.5511\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.5993 - accuracy: 0.5523\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.5929 - accuracy: 0.5485\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.5857 - accuracy: 0.5540\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.5695 - accuracy: 0.5570\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.5607 - accuracy: 0.5582\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.5644 - accuracy: 0.5593\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5562 - accuracy: 0.5594\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5413 - accuracy: 0.5654\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.5431 - accuracy: 0.5638\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.5435 - accuracy: 0.5597\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5245 - accuracy: 0.5662\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 8s 166us/sample - loss: 1.5285 - accuracy: 0.5685\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5281 - accuracy: 0.5670\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5157 - accuracy: 0.5694\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5099 - accuracy: 0.5699\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.5021 - accuracy: 0.5756\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.4999 - accuracy: 0.5744\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.4929 - accuracy: 0.5757\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.4920 - accuracy: 0.5741\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4818 - accuracy: 0.5776\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.4759 - accuracy: 0.5791\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.4694 - accuracy: 0.5824\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.4698 - accuracy: 0.5796\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4635 - accuracy: 0.5815\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.4629 - accuracy: 0.5807\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.4451 - accuracy: 0.5890\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.4465 - accuracy: 0.5847\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.4441 - accuracy: 0.5864\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.4429 - accuracy: 0.5859\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4339 - accuracy: 0.5877\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.4274 - accuracy: 0.5923\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.4224 - accuracy: 0.5914\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4131 - accuracy: 0.5937\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4088 - accuracy: 0.5967\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.4160 - accuracy: 0.5959\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.4157 - accuracy: 0.5940\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4008 - accuracy: 0.5991\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.4001 - accuracy: 0.5981\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.3893 - accuracy: 0.5989\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.3854 - accuracy: 0.5977\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.3886 - accuracy: 0.6007\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.3766 - accuracy: 0.6027\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3840 - accuracy: 0.6024\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3732 - accuracy: 0.6040\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.3665 - accuracy: 0.6038\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3654 - accuracy: 0.6056\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3637 - accuracy: 0.6055\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.3549 - accuracy: 0.6078\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.3609 - accuracy: 0.6044\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.3549 - accuracy: 0.6099\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.3465 - accuracy: 0.6111\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.3478 - accuracy: 0.6100\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.3426 - accuracy: 0.6110\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.3370 - accuracy: 0.6151\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.3289 - accuracy: 0.6132\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3241 - accuracy: 0.6155\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.3219 - accuracy: 0.6167\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.3265 - accuracy: 0.6147\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.3225 - accuracy: 0.6122\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.3185 - accuracy: 0.6170\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3137 - accuracy: 0.6181\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.3094 - accuracy: 0.6190\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3012 - accuracy: 0.6241\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3025 - accuracy: 0.6205\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3017 - accuracy: 0.6221\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.2928 - accuracy: 0.6223\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.2957 - accuracy: 0.6234\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2945 - accuracy: 0.6226\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.2894 - accuracy: 0.6247\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.2812 - accuracy: 0.6265\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2818 - accuracy: 0.6241\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.2775 - accuracy: 0.6271\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2674 - accuracy: 0.6293\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2726 - accuracy: 0.6276\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.2618 - accuracy: 0.6303\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.2688 - accuracy: 0.6298\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2623 - accuracy: 0.6324\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2511 - accuracy: 0.6352\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.2569 - accuracy: 0.6307\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.2570 - accuracy: 0.6342\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2594 - accuracy: 0.6302\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.2516 - accuracy: 0.6341\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2465 - accuracy: 0.6340\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.2402 - accuracy: 0.6386\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2398 - accuracy: 0.6345\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2382 - accuracy: 0.6351\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 8s 162us/sample - loss: 1.2325 - accuracy: 0.6410\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2288 - accuracy: 0.6387\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2328 - accuracy: 0.6361\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.2345 - accuracy: 0.6397\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 1.2173 - accuracy: 0.6407\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2282 - accuracy: 0.6388\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.2197 - accuracy: 0.6415\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.2137 - accuracy: 0.6450\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.2112 - accuracy: 0.6408\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2117 - accuracy: 0.6434\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.2018 - accuracy: 0.6469\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2082 - accuracy: 0.6431\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.2044 - accuracy: 0.6455\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.2013 - accuracy: 0.6471\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU5b348c83k5WskIQkJIEQw5qw\nySKgoOJuLa5taV3A5draWmu9avH6a696a8Uut7V6q3XHHXdsUamiAioKIRCUTbYAYclGNrJP8v39\nMSdpCAmyZDID832/XvPKzHOec+Y7Zybzned5znmOqCrGGGMCV5CvAzDGGONblgiMMSbAWSIwxpgA\nZ4nAGGMCnCUCY4wJcJYIjDEmwFkiMMaYAGeJwPgNEfmRiOSKyH4R2SMi74nIac6ye0TkhXZ1VURq\nnLr7RaSiw7aynDoPdygP7rBuoYj8QUS6/F8QkV+IyEoRaRSRJztZfq6IbBSRWhH5SET6t1sWLiLP\nikiV85p+8S37IFVEnhGRvc4660Xkv0UkwlkeJCK/EpHNIlInIttF5H4RCW23jRec13hyu7KhIuJ2\n7j8pIk938txjRaReROIOFaM58VgiMH5BRG4D/gL8DkgC+gN/Ay4+xGqjVDXKuXX88poJ7ANmiEhI\nJ+tmq2oUMA242qnflV3AfcCzncSdBLwO3AXEA6uBl9pV+R8gw3k95wD/JSJnd/YkIpIALAOCgVNU\nNQY4H0gEMp1q/wdcB1wJRAPfAc4DXumwuX3Ab7t4PXOBK1qTSztXA/NVtaKTdcyJTFXtZjef3oBY\nYD/wvUPUuQd4od1jBbK6qCtAAXAjUApc0m5ZsLNuRruyN4GHDiPOOcCTHcp+Cixp9zgGaGiNDSgC\nprVb/kD719HJ9lcB0sXyoUALcHKH8gygEZjqPH4B+CNQDJzabl13u/2zBfhRh/1SBHzH158Hu/X8\nzVoExh9MAsKBt7ppe2fgaVW8ArzGIX7ti8gw4FRg81E+VzaQ3/pAVauAbUC2iCQCfdsvd+5nd7Gt\ns4E31flm7mL5NlXNa1+oqgXACjwtjlb78SSW+ztuxNn+c8A17YrPw5MgF3bx3OYEZonA+IN4oFRV\n3Ue4Xp6IVDi3v7YrnwkscL6UXwIuFJH4DuuuEZEaYB3wAfD3o4w9CqjsUFaJp9smqt3jjss6Ew/s\nOcRzJRxi+R5neXt/AwaJyDmd1H8OOEtEUpzH1wAvHsV7YE4AlgiMPygDEkQk+AjXO1lV45zbLQAi\nEglcDrzo1PkU2Av8sMO6I/F8If8IT4sk8ihj34+nO6i9GKDaWUaH5a3LOlMGpHSxDDzdXF0tT3GW\nt1HVejzjBP/TsbKqbgM+B64UkVhgOp7kYAKQJQLjD5bh6Ve/pBu2dTmeX+KPi8hePL+Uk+ike0hV\nW1T1ZSAXuPson28tMKr1gYhEAwOBtapaApS0X+7cX9vFtj4ELhUR6WL5ImBg+6OBnOfMAMY7yzt6\nEk/31PROls3FM0B8BbBRVfM7qWMCgCUC43OqWgn8Bvg/EblERHqJSIiIXCAivz/Czc0EngBGAKOd\n21RgrDMe0Jk5wE+cPv2DOIechgMuwOUcEupyFr8BjHbiDgf+G8hV1dYxh+eAX4tInIgMx3PEz7Nd\nxPFHPN07z7QegioiaSLykIhkq+p6PF/sL4vIBBFxiUgOnqOW3lPVTzpuUFWbgHuBX3XyfK8BWcCv\n8SQFE6AsERi/oKp/Am4D/h+eX9E7gZuBtw93G86X5xnAX1R1b7vbcjy/tjsdNFbVVXhaJbd3sel7\ngDpn+Szn/l3OukXA94HfA+XAyXi6m1r92nktO4GPgAdU9cMu4ijF000FsEJEqvGMX5QCW53ym/B8\nab8M1ADvOXW+30Xs4DmKqLiT56vGM0CfyoGHvJoAI10foGCMMSYQWIvAGGMCnCUCY4wJcJYIjDEm\nwFkiMMaYAHekJ/D4XEJCgmZkZPg6DGOMOa6sXLmyVFU7PUT6uEsEGRkZ5Obm+joMY4w5rojI9q6W\nWdeQMcYEOEsExhgT4CwRGGNMgDvuxgiM6W5NTU0UFhZSX1/v61CMOWbh4eGkpaUREtLZhfk6Z4nA\nBLzCwkKio6PJyMig64k/jfF/qkpZWRmFhYUMHDjwsNezriET8Orr64mPj7ckYI57IkJ8fPwRt24t\nERgDlgTMCeNoPssBkwg27q3mjws3Ura/wdehGGOMXwmYRLC1ZD+PfLyZoipLBMb/REVFfXulHnDd\nddfRt29fcnJyuqzz4osvMnLkSEaMGMHkyZPJz//3hc3ef/99hgwZQlZWFnPmzGkrf+SRR8jKykJE\nKC0tPaxt/fnPfyY7O5ucnBx++MMftnV3XHnllQwZMoScnByuu+46mpqaAKisrOS73/0uo0aNIjs7\nm2eeeaZtW3feeSfZ2dkMGzaMW265hdbp91euXMmIESPIyso6oHzfvn2cc845DBo0iHPOOYfy8nIA\nysvLufTSSxk5ciQTJkzg66+/bnuOiooKrrjiCoYOHcqwYcNYtmzZAfvtT3/60wGvf/78+YwcOZLR\no0czbtw4Pv3007a6LpeL0aNHM3r0aKZP//fF5WbNmsXAgQPblq1evbrL9+mIqOpxdRs7dqwejY83\nFOmAX/1TcwvKjmp9c+Jat26dr0PQyMhIX4egqqqLFy/WlStXanZ2dpd1PvvsM923b5+qqr777rs6\nYcIEVVV1u92amZmpW7Zs0YaGBh05cqSuXbtWVVXz8vJ027ZtOmDAAC0pKfnWbRUWFmpGRobW1taq\nqur3vvc9feaZZ1RVdcGCBdrS0qItLS06Y8YM/dvf/qaqqvfff7/eeeedqqpaXFysvXv31oaGBv3s\ns8908uTJ6na71e1268SJE/Xjjz9WVdXx48frsmXLtKWlRc8//3x99913VVX1jjvu0AceeEBVVR94\n4IG27d5+++16zz33qKrq+vXrddq0aW2v5ZprrtEnnnhCVVUbGhq0vLy8bdmOHTv03HPP1f79+7e9\n/urqam1paVFV1fz8fB0yZEhb/a4+DzNnztTXXnuty/emVWefaTxXzuv0ezVgWgS9Qj0HSNU2Nvs4\nEmO6pqrccccd5OTkMGLECObNmwfAnj17mDp1KqNHjyYnJ4elS5fS3NzMrFmz2ur++c9/Pubnnzp1\nKn369DlkncmTJ9O7d28AJk6cSGFhIQDLly8nKyuLzMxMQkNDmTFjBvPnzwdgzJgxdDZHWFfbAnC7\n3dTV1eF2u6mtraVfv34AXHjhhYgIIsKECRPa1hERqqurUVX2799Pnz59CA4ORkSor6+nsbGRhoYG\nmpqaSEpKYs+ePVRVVTFx4kREhGuuuYa33/ZcEG/+/PnMnOm5oN3MmTPbytetW8e0adMAGDp0KAUF\nBRQVFVFZWcmSJUu4/vrrAQgNDSUuLq7ttfzyl7/k97///QH991FRUW2Pa2pqfDpOFTCHj/YK9Vxi\n1hKBOZR7/7GWdburunWbw/vF8N/fzT6sum+++SarV68mPz+f0tJSxo8fz9SpU3nppZc477zzuPvu\nu2lubqa2tpbVq1eza9eutu6JioqKg7b34osv8oc//OGg8qysLF5//fVje2HAU089xQUXXADArl27\nSE9Pb1uWlpbGl19+eVTbSk1N5fbbb6d///5ERERw7rnncu655x5Qv6mpieeff56HHnoIgJtvvpnp\n06fTr18/qqurmTdvHkFBQUyaNIkzzzyTlJQUVJWbb76ZYcOGkZubS1pa2gHx7tq1C4CioiJSUlIA\nSE5OpqioCIBRo0bx5ptvMmXKFJYvX8727dspLCzE5XKRmJjItddeS35+PmPHjuWhhx4iMjKS+fPn\nk5qayqhRow56zW+99RZ33XUXxcXFLFiwoK28vr6ecePGERwczOzZs7nkkkvalt19993cd999nHXW\nWcyZM4ewsLDD3sddCZgWQYSTCOosERg/9umnn/LDH/4Ql8tFUlISp59+OitWrGD8+PE888wz3HPP\nPXz11VdER0eTmZnJ1q1b+fnPf877779PTEzMQdu78sorWb169UG37kgCH3/8MU899RQPPvhgt2+r\nvLyc+fPns23bNnbv3k1NTQ0vvPDCAev89Kc/ZerUqUyZMgWAhQsXMnr0aHbv3s3q1au5+eabqaqq\nYvPmzaxfv57CwkJ27drFRx99xNKlSw87ttbWB8Ds2bOpqKhg9OjRPPzww4wZMwaXy4Xb7SYvL4+b\nbrqJVatWERkZyZw5c6itreV3v/sd9913X6fbvvTSS9mwYQNvv/02v/71r9vKt2/fTm5uLi+99BK3\n3norW7ZsAeCBBx5gw4YNrFixgn379nXLvocAahFEWteQOQyH+8u9p02dOpUlS5awYMECZs2axW23\n3cY111xDfn4+Cxcu5LHHHuPVV1/l6aefPmA9b7UI1qxZww033MB7771HfHw84PkVv3PnzrY6hYWF\npKamHtW2PvzwQwYOHEhiomfW5Msuu4zPP/+cq666CoB7772XkpIS/v73v7dt55lnnmH27NmICFlZ\nWQwcOJANGzawePFiJk6c2DYgf8EFF7Bs2TKuvvrqA7qi2sfb2nWUkpLCnj176Nu3LwAxMTFtg9Cq\nysCBA8nMzKS2tpa0tDROOeUUAK644grmzJnDli1b2LZtW1troLCwkJNPPpnly5eTnJzc9txTp05l\n69atlJaWkpCQ0BZHZmYmZ5xxBqtWreKkk05qa6WEhYVx7bXX8sc//vEw3q1vF3AtgtpGt48jMaZr\nU6ZMYd68eTQ3N1NSUsKSJUuYMGEC27dvJykpif/4j//ghhtuIC8vj9LSUlpaWrj88sv57W9/S15e\n3kHb80aLYMeOHVx22WU8//zzDB48uK18/PjxbNq0iW3bttHY2Mgrr7xywBEvR7Kt/v3788UXX1Bb\nW4uqsmjRIoYNGwbAk08+ycKFC3n55ZcJCgo6YJ1FixYBnq6djRs3kpmZSf/+/Vm8eDFut5umpiYW\nL17MsGHDSElJISYmhi+++AJV5bnnnuPiiy8GYPr06cydOxeAuXPntpVXVFTQ2NjYFsfUqVOJiYkh\nOTmZ9PR0Nm7cCMCiRYsYPnw4I0aMoLi4mIKCAgoKCkhLSyMvL4/k5GQ2b97cdpRSXl4eDQ0NxMfH\nU15eTkOD5+jG0tJSPvvsM4YPHw54xorAk4TefvvtQx7ddUS6GkX219vRHjXU6G7WAb/6p/71w2+O\nan1z4vKno4ZaWlr09ttv1+zsbM3JydFXXnlFVVWfffZZzc7O1tGjR+tpp52mW7du1dWrV+uYMWN0\n1KhROmrUqLYjXo7FjBkzNDk5WYODgzU1NVWffPJJVVV99NFH9dFHH1VV1euvv17j4uLanrf9/+SC\nBQt00KBBmpmZqb/97W/byh966CFNTU1Vl8ulKSkpev3113/rtn7zm9/okCFDNDs7W6+66iqtr69X\nVVWXy6WZmZlt69x7772qqrpr1y4955xzNCcnR7Ozs/X5559XVc/RTDfeeKMOHTpUhw0bpr/85S/b\nnmPFihWanZ2tmZmZ+rOf/aztKJ7S0lKdNm2aZmVl6VlnnaVlZZ6jDT///HMdNGiQDh48WC+99NK2\nI55UVVetWqVjx47VESNG6MUXX3zAslbtj5qaM2eODh8+XEeNGqUTJ07UpUuXqqrnSKqcnBwdOXKk\n5uTktL0Hqqpnnnlm2+u78sortbq6utP38UiPGhJ1MtLxYty4cXq0F6YZdPe7XH9aJrMvGNrNUZnj\n2fr169t+bRpzIujsMy0iK1V1XGf1A6ZrCCAixEWddQ0ZY8wBAioR9AoNtsFiY4zpILASQZiL2iZL\nBOZgx1sXqTFdOZrPcmAlglCXnUdgDhIeHk5ZWZklA3PcU+d6BOHh4Ue0XsCcRwDQKyTYDh81B0lL\nS6OwsJCSkhJfh2LMMWu9QtmR8HoiEBEXkAvsUtWLOiybBfwB2OUUPaKqT3orlohQFxW1jd7avDlO\nhYSEHNHVnIw50fREi+AXwHrg4PPfPeap6s09EAe9Ql3srrCuIWOMac+rYwQikgZ8B/Dar/wjERHq\nsqOGjDGmA28PFv8FuBNoOUSdy0VkjYi8LiLpnVUQkRtFJFdEco+lH7dXqMvGCIwxpgOvJQIRuQgo\nVtWVh6j2DyBDVUcCHwBzO6ukqo+r6jhVHdc6CdXRiLTzCIwx5iDebBGcCkwXkQLgFWCaiBwwj6yq\nlqlq67UjnwTGejEeIkJdNLhbaG6xwwSNMaaV1xKBqt6lqmmqmgHMAD5S1ava1xGRlHYPp+MZVPaa\n1ovT1NlJZcYY06bHzyMQkfvwzIL3DnCLiEwH3MA+YJY3nzui7ZoEbqLCAuoUCmOM6VKPfBuq6ifA\nJ87937Qrvwu4qydiAOgVYlcpM8aYjgJuigmAmgZLBMYY0yqgEkHbdYub7BBSY4xpFVCJIDLMrlts\njDEdBVQiiAhpvW6xJQJjjGkVUImg7fBRSwTGGNMmwBKBdQ0ZY0xHAZUIWgeLbb4hY4z5t4BKBNY1\nZIwxBwuoRBDiCiLEJdRYIjDGmDYBlQjAM05QZ11DxhjTJgATgV2cxhhj2gu4RBAR6qLWZh81xpg2\nAZcIosOCqa63riFjjGkVcImgb0w4RZX1vg7DGGP8RsAlgpTYcPZWWSIwxphWAZcIkmLCqaxrsnMJ\njDHGEXCJICU2HMBaBcYY4wi4RJAc40kEeyrrfByJMcb4B68nAhFxicgqEflnJ8vCRGSeiGwWkS9F\nJMPb8SQ7LYIiaxEYYwzQMy2CXwDru1h2PVCuqlnAn4EHvR1MayLYY0cOGWMM4OVEICJpwHeAJ7uo\ncjEw17n/OnCWiIg3Y+oVGkxMeLAdQmqMMQ5vtwj+AtwJtHSxPBXYCaCqbqASiO9YSURuFJFcEckt\nKSk55qCSY8OtRWCMMQ6vJQIRuQgoVtWVx7otVX1cVcep6rjExMRjji05NsLGCIwxxuHNFsGpwHQR\nKQBeAaaJyAsd6uwC0gFEJBiIBcq8GBMAyTFh1iIwxhiH1xKBqt6lqmmqmgHMAD5S1as6VHsHmOnc\nv8Kpo96KqVVybAQl+xtoau6qx8oYYwJHj59HICL3ich05+FTQLyIbAZuA2b3RAzJMeGoQkl1Q088\nnTHG+LXgnngSVf0E+MS5/5t25fXA93oihvZazy7eXVFHv7iInn56Y4zxKwF3ZjFARkIkANtKa3wc\niTHG+F5AJoL03hGEuoLYXLLf16EYY4zPBWQiCHYFkZHQiy3F1iIwxpiATAQAJyVGsdVaBMYYE9iJ\nYPu+WhrddgipMSawBW4i6BtJc4uyY591DxljAlvgJoLEKAA2F1v3kDEmsAVsIsh0EsGWEmsRGGMC\nW8AmgqiwYFJiw9liLQJjTIAL2EQAkNU3im+Kq30dhjHG+FRAJ4JhKTF8U7TfJp8zxgS0gE4Ew1Ni\naHS3sNXGCYwxASywE0G/GADW7an0cSTGGOM7AZ0IMhMiCQ0OYt3uKl+HYowxPhPQiSDYFcSQpGjW\n7bFEYIwJXAGdCMAzTrB+TzU9cGE0Y4zxS5YI+sWwr6aRoiq7WpkxJjAFfCLIdgaMv9plA8bGmMDk\ntUQgIuEislxE8kVkrYjc20mdWSJSIiKrndsN3oqnKzmpsYS4hNzt+3r6qY0xxi9485rFDcA0Vd0v\nIiHApyLynqp+0aHePFW92YtxHFJ4iIsRqbGsLCj3VQjGGONTXmsRqEfrRD4hzs0vR2THZ/RhTWEl\n9U3Nvg7FGGN6nFfHCETEJSKrgWLgA1X9spNql4vIGhF5XUTSvRlPV8YO6E1jc4uNExhjApJXE4Gq\nNqvqaCANmCAiOR2q/APIUNWRwAfA3M62IyI3ikiuiOSWlJR0e5xjB/QGYEWBjRMYYwJPjxw1pKoV\nwMfA+R3Ky1S19bjNJ4GxXaz/uKqOU9VxiYmJ3R5ffFQYJyVGkmvjBMaYAOTNo4YSRSTOuR8BnANs\n6FAnpd3D6cB6b8XzbSZmxvPl1jIa3DZOYIwJLN5sEaQAH4vIGmAFnjGCf4rIfSIy3alzi3NoaT5w\nCzDLi/Ec0lnD+lLT2MzybdY9ZIwJLF47fFRV1wBjOin/Tbv7dwF3eSuGIzEpM4Gw4CAWrS9myqDu\n734yxhh/FfBnFreKCHVxalYCizYU2bxDxpiAYomgnWlD+7JzXx1bSuw6xsaYwGGJoJ1pQ/sCsGh9\nsY8jMcaYnmOJoJ1+cREMS4nhow2WCIwxgcMSQQfThiaSu72cytomX4dijDE9whJBB9OGJtHcoize\n1P1nMBtjjD+yRNDB6PQ4+kSG8tH6Il+HYowxPcISQQeuIOHMIX35aEOxnWVsjAkIlgg6cdHIFKrq\n3Sz5ptTXoRhjjNdZIujEaYMS6N0rhHfyd/s6FGOM8TpLBJ0IcQVxwYgUPlxXRG2j29fhGGOMV1ki\n6ML0Uf2oa2rmQzu5zBhzgrNE0IUJGX1IjgnnndXWPWSMObFZIuhCUJBw0cgUFn9TbCeXGWNOaJYI\nDmH66H40NSvvr93j61CMMcZrLBEcwojUWDLie9nRQ8aYE9phJQIR+YWIxIjHUyKSJyLnejs4XxMR\npo/qx7ItZRRX1/s6HGOM8YrDbRFcp6pVwLlAb+BqYI7XovIj00f3o0VhwRrrHjLGnJgONxGI8/dC\n4HlVXduu7ISW1TeaYSkx1j1kjDlhHW4iWCki/8KTCBaKSDTQcqgVRCRcRJaLSL5zgfp7O6kTJiLz\nRGSziHwpIhlH+gJ6wvRR/Vi1o4Kd+2p9HYoxxnS7w00E1wOzgfGqWguEANd+yzoNwDRVHQWMBs4X\nkYmdbLdcVbOAPwMPHnbkPei7o1IAeG1loY8jMcaY7ne4iWASsFFVK0TkKuD/AZWHWkE9Wi/+G+Lc\nOl4V/mJgrnP/deAsEfG7Lqe03r04e1gScz8voKbBppwwxpxYDjcRPArUisgo4D+BLcBz37aSiLhE\nZDVQDHygql92qJIK7ARQVTee5BLfyXZuFJFcEcktKfHNBWN+euZJVNY18fLyHT55fmOM8ZbDTQRu\nVVU8v+AfUdX/A6K/bSVVbVbV0UAaMEFEco4mSFV9XFXHqeq4xMTEo9nEMTu5f28mZvbhiaVbqW+y\n6xQYY04ch5sIqkXkLjyHjS4QkSA8XT2HRVUrgI+B8zss2gWkA4hIMBALlB3udnvaLWcNoqiqgRe+\n2O7rUIwxptscbiL4AZ7B3+tUdS+eX/h/ONQKIpIoInHO/QjgHGBDh2rvADOd+1cAHzktD780+aQE\nTstK4G+fbGG/jRUYY04Qh5UInC//F4FYEbkIqFfVbxsjSAE+FpE1wAo8YwT/FJH7RGS6U+cpIF5E\nNgO34Tkyya/dcd4Q9tU08tTSbb4OxRhjukXw4VQSke/jaQF8gudEsodF5A5Vfb2rdVR1DTCmk/Lf\ntLtfD3zvCGP2qVHpcZyXncQTS7dy9aQB9IkM9XVIxhhzTA63a+huPOcQzFTVa4AJwK+9F5Z/u/3c\nIdQ2unls8RZfh2KMMcfscBNBkKq2v1RX2RGse8IZlBTNpWPSmPt5AXsrbTI6Y8zx7XC/zN8XkYUi\nMktEZgELgHe9F5b/u/XsQbSo8tePNvk6FGOMOSaHO1h8B/A4MNK5Pa6qv/JmYP4uvU8vfjShP6+u\n2ElBaY2vwzHGmKN22N07qvqGqt7m3N7yZlDHi59NyyLEFcR9/1yHHx/1aowxh3TIRCAi1SJS1cmt\nWkSqeipIf9U3Opw7zx/CRxuKmbdip6/DMcaYo3LIRKCq0aoa08ktWlVjeipIfzZzUgaTT4rnf/65\njj2Vdb4OxxhjjljAHvnTXYKChAcvH0lTi/L79zf6OhxjjDlilgi6QXqfXvzHlIG8tWoXq3aU+zoc\nY4w5IpYIuslNZ2TRNzqMn7+8it0V1kVkjDl+WCLoJlFhwTw5cxyVtU386IkvqKht9HVIxhhzWCwR\ndKORaXE8e914CsvruOedtb4OxxhjDoslgm42dkAfbp6Wxdurd/P+13t9HY4xxnwrSwRe8LMzs8hJ\njeGO1/L5pqja1+EYY8whWSLwghBXEH+/ehzhoS6ufWYFpfsbfB2SMcZ0yRKBl6TGRfD0zPGU7m/g\nlpdX0dxiU1AYY/yTJQIvGpEWy/9cksPnW8p48P0NNh+RMcYvHdYVyszR+/64dL4qrOTxJVsJDhLu\nOG8IIuLrsIwxpo3XEoGIpAPPAUmA4pm6+qEOdc4A5gOtFwB+U1Xv81ZMvnLv9GyaVfnbJ1voFeri\n5mmDfB2SMca08WaLwA38p6rmiUg0sFJEPlDVdR3qLVXVi7wYh88FBQn3X5JDXWMzf/zXN6T2juDS\nMWm+DssYYwAvJgJV3QPsce5Xi8h6IBXomAgCgohncrq9lfX856v5AJYMjDF+oUcGi0UkAxgDfNnJ\n4kkiki8i74lIdk/E4yuhwUE8OXMcpwyM57ZX83l5+Q5fh2SMMd5PBCISBbwB3KqqHS9mkwcMUNVR\nwMPA211s40YRyRWR3JKSEu8G7GWRYcE8c+14Th+cyF1vfsWzn2379pWMMcaLvJoIRCQETxJ4UVXf\n7LhcVatUdb9z/10gREQSOqn3uKqOU9VxiYmJ3gy5R4SHuPj71WM5d3gS9/xjHW+sLPR1SMaYAOa1\nRCCeYySfAtar6v92USfZqYeITHDiKfNWTP4kLNjFwz8aw6lZ8dz5xhr+/ME3NmOpMcYnvNkiOBW4\nGpgmIqud24Ui8hMR+YlT5wrgaxHJB/4KzNAAOusqLNjF368exznDknho0SbO/t/FbNgb8JeCNsb0\nMDnevnfHjRunubm5vg6j2329q5Lr566g0d3CU7PGc3L/3r4OyRhzAhGRlao6rrNlNsWEn8hJjWXe\njZOIDAvme48t4+FFm2x+ImNMj7BE4EcyEiJ59xdTuGhkCn/64Bt+8Pdl7LLLXhpjvMwSgZ+JCQ/h\noRlj+MsPRrNxbzWX/e0zGzcwxniVJQI/dcmYVF6/aTKC8L1Hl/HeV3t8HZIx5gRlicCPDUmO5o2f\nTiazbxQ3vZjHz17KY91uax0YY7qXJQI/lxoXwWs/nsQtZw1i8cYSLnp4KfNW2NQUxpjuY4ngOBAa\nHMRt5wzms19NY8qgRH71xgw3BrAAABTmSURBVFfMeW8DdY3Nvg7NGHMCsERwHIntFcIT14xjxvh0\nHlu8xU5AM8Z0C0sEx5nQ4CDmXD6SV388ieYWZcbjX/DJxmI758AYc9QsERynJgzsw6s/nkR0eDCz\nnlnBxAcW8cIX2y0hGGOOmE0xcZyrbXTz0YZinlu2neXb9jEwIZLvjUtj1uQMeoXaJamNMR6HmmLC\nEsEJQlV596u9zP28gOUF+8hMiOQvM0YzMi3O16EZY/yAJYIAs2xLGb+ct5qi6np+MC6d749PZ2Rq\nLMEu6wk0JlBZIghAlXVN/HXRJuZ+XoC7RRmYEMmz145nQHykr0MzxviAzT4agGIjQvj1RcNZfvfZ\n/OUHoymvbeTyR5fxr7V7abEBZWNMOzaaeILrExnKJWNSyUmN4Ya5udz4/Eoy4ntx+uBErpw4gMFJ\n0b4O0RjjY9Y1FEDczS38Y81u5q/ezRdby2hqVmZOyuCO84YQEerydXjGGC86VNeQtQgCSLAriEvH\npHHpmDT21TTyp39t5OnPtrFkUwmzzx/K1MGJhAZbb6Exgcb+6wNUn8hQ7r90BM9fP4Hq+iZueC6X\nyXM+4p9rdnO8tRKNMcfGa11DIpIOPAckAQo8rqoPdagjwEPAhUAtMEtV8w61Xesa6n6N7haWbirh\noUWbWFNYSWZiJDn9YgE4NSue749Lx/NWGWOOV77qGnID/6mqeSISDawUkQ9UdV27OhcAg5zbKcCj\nzl/Tg0KDgzhrWBKnD07k5RU7+XhDMXk7ynE3K+/k7+Yf+Xt48IqRpMZF+DpUY4wX9NhgsYjMBx5R\n1Q/alf0d+ERVX3YebwTOUNUuL8dlLYKeo6q8tHwH9y9YT5AIt549iBkT+hMVZkNLxhxvfH4egYhk\nAGOALzssSgV2tntc6JR1XP9GEckVkdySkhJvhWk6EBGuPGUAC2+dyuj0OH67YD2THljE/QvWsbui\nztfhGWO6idd/2olIFPAGcKuqHtXk+ar6OPA4eFoE3RieOQzpfXrxwg2nsGpHOU99uo2nPyvg2c8L\nuGJsOmcMSeTk/r1JjA7zdZjGmKPk1UQgIiF4ksCLqvpmJ1V2AentHqc5ZcYPjenfm0d+1JvC8loe\nW7yFV1cU8vLyHQQJTB2cyOwLhjI0OcbXYRpjjpA3jxoSYC6wT1Vv7aLOd4Cb8Rw1dArwV1WdcKjt\n2hiB/6hrbGbD3ioWrS/m5eU72N/gZubkDMKDgxiUFM3kk+KJj7KWgjH+wCeTzonIacBS4CugxSn+\nL6A/gKo+5iSLR4Dz8Rw+eq2qHvJb3hKBfyqpbuA/X8tnyTf/HsPpFeri/ktzuHRMmg8jM8aAzT5q\nelBzi6KqfL27it8tWM/ygn2EhwRxUmIUv75oOBMz430dojEByRKB8Ql3cwuvrSxka8l+3l+7l537\n6hiVHse4Ab35wfh0m/DOmB5kicD4XG2jm6eWbuOzLaXk7aig0d3C2cP6MvuCYWT1jfJ1eMac8CwR\nGL+yr6aRl77czmOLt7K/wU1a7whGpceR3S+G3r1CGZ/Rx5KDMd3MEoHxSyXVDby1qpD8nZXkF1ZQ\nWO45SS3EJdx0RhbDU6LJTIyyLiRjuoFNQ238UmJ0GDdOPantcU2Dm9L9Dfx+4Ub+umhTW/nItFju\nOG8IUwYl+iJMY0541iIwfml7WQ3V9W5WFOzj2c8L2F5Wy6TMeMb0jyMxOoyTEqM4LSuBoCCbFdWY\nw2FdQ+a4Vt/UzBNLtrLgqz1sKt5Ps3PN5UF9o/jx6ScxfVQ/u6COMd/CEoE5YTQ1t1Bd72bJNyU8\ntngLG/ZWEx0WzKCkKC47OY0pgxJ4NXcn04b2ZeyAPr4O1xi/YYnAnJBUlcXflPDh+iJW76zg613/\nntMwIsTFnMtHUFRVT1bfKM4c0tcurmMCmiUCc8JTVT7aUMz6PVWcPrgvt85bxZaSmrblo9JiOXtY\nEt8d1Y+MhEgfRmqMb1giMAGndH8DSzeVMGFgPEu+KeG5ZdtZv6eKEJdw9cQMpgxKYEB8L5Jiwom0\nC+2YAGCJwBigqKqePy7cyOt5hbR+7EVgRGos2f1iiAwN5oIRKZzcP866kcwJxxKBMe1U1jaxsaia\nXRW1bC+rZemmUnbsq6WqrokGdwvZ/WK4dEwqPxifTnR4iK/DNaZbWCIw5jDUNLh5I6+Q13IL+WpX\nJX0iQ7lwRDL76930iQwjq28UEzP7MDAh0loM5rhjicCYI7SmsIIH39/Amp2VxEWGUFrdSF1TMwBx\nvULI6RdLdmoM3x3Zj5zUWB9Ha8y3s0RgzDFSVbaV1vDF1n2sKazg692VbNxbTVOzcsrAPkSHB6MK\n4SEuxvSP45SB8QzvF4PLznw2fsLmGjLmGIkImYlRZCZG8aNT+gNQVd/EU0u3sWhDEdX1bkQ8ZQu+\n2gN4Wg43n5nFNZMy7Mxn49esRWBMN9tbWc+X28p4I28XS74pIT4ylMlZCVTWNREV5mLa0CQuGplC\neIjL16GaAOKraxY/DVwEFKtqTifLzwDmA9ucojdV9b5v264lAnM8WfxNCW+sLGRFwT4SosIorq6n\nqKqBlNhwLhqZQlOzUtfYTHqfCG6YkmnJwXiNr7qGnsVzYfrnDlFnqape5MUYjPGp0wcncvrgf0+f\nraos21LGH/+1kWc/LyAixEVEqIuiqgbeWrWLfnER7KtpJNgVxNRBCVw1cQBJMeE+fAUmEHgtEajq\nEhHJ8Nb2jTkeiQiTsxJ4MyvhgPJPNhYz570NVNQ2kRwTTnWDm0c+3szDH20mvU8EI1PjGJkWy8i0\nOHJSY+z8BtOtfD1YPElE8oHdwO2qutbH8RjjE2cM6csZQ/oeULattIaFa/fyVaHnCm6tg9AikNY7\ngsjQYIb3i+Hi0amMz+hNr1Bf/zub45VXB4udFsE/uxgjiAFaVHW/iFwIPKSqg7rYzo3AjQD9+/cf\nu337dq/FbIy/2lfTyJrCCr4qrGRT8X5qGtwsL9hHdb0bV5AwNDmakWmxhLiC6BMZyuj0OGIiQhjQ\npxfxUWG+Dt/4mM/OIzhUIuikbgEwTlVLD1XPBouN+bf6pmaWbSlj5fZy8naUs2FvNS2qVNY1tc2n\n1CvUxS/PHsyQ5GhaVImJCCEmPITk2HCibMK9gOGX5xGISDJQpKoqIhOAIKDMV/EYczwKD3Fx5tC+\nnDn0wG6lqvom1u+uoqbRzXPLtnP/u+sPWjc4SJh0UjynZSUwdkBvclJj7ailAOW1RCAiLwNnAAki\nUgj8NxACoKqPAVcAN4mIG6gDZujxdlKDMX4qJjyEUzLjAThzSF/yCytxN7c4J725qaprYt2eKj5Y\nV8QD720AIMQlZPeLZeyA3ozpH0dxVQPr93gu9jN+YB8uG5NKsMtOjDsR2QllxgS40v0NrNpR4ele\n2l5OfmEFDe4WABKjw1D11EmNiyAlNpzBydFcfnIq2f1icQUJVXVNNgZxHLC5howxh63R3cLGvdX0\njgwhrXcvVJWFa4t4fWUhNQ1uVu+saJuATwRUYXR6HIOTovhqVxXjBvTmkjGpbQPXxj9YIjDGdJvq\n+iYWf1PCtpIa3C1KaHAQb6wspGR/A9n9YsjbUUGju4XwkCDiI8OICgsmKjyYMelxDEuJYWNRNb17\nhXJy/zjGDuht3U09xBKBMcbrVBURobK2ic+2lLJyeznlNY3sb3BTUdvEqp3lNDUroa4gGps9XU8x\n4cFt3U+Dk6LJ7hfDiLRYJgzsY+dFdDO/PGrIGHNiab1YT2yvEC4ckcKFI1IOWF5V38TuijpOSoyi\nut7Nl1vL+GRjCfsb3bS0KBv2VvP+2r0AhAYHMTI1lkFJ0SRGh1HT4GbnvlqSY8PJTIjkpL6emWBT\nYsIJsqm+j5m1CIwxfqO6von8nZV8srGY/MIKtpTUUF7bSKgriLTeERRXNVDd4G6rHx4SRGZCFBMz\n4xmVHktDk+fIqIToMCZlxhMe4qKlRS1ZYF1DxpjjWHOLIkBQkKCqlFQ3sKWkhq2l+9lSXMM3RdUs\nL9hHo3OkU6vwkCBCgoKobnC3JZLs1FimDkqgf59eiAgD4nvRNzosIC49al1DxpjjVvurvIkIfWPC\n6RsTzqST4tvKaxvd7CqvazshrqCsho82FKPquUBQXVMzBaU1fLm1jH/k7z5g++l9IjhraBLF1fVE\nh4Vw/ohkdpTVUt/UzLiM3oxO733QleZOtFaGtQiMMQFDVVm/p5ry2kbcLcq2kv0s2lDMsi1lpPaO\noLS6gZrG5gPWSY2LICc1ho17q6lraqbB3UJFbRNZfaM4LSuBUemxjEiNIzMh0q+Tg3UNGWPMIbQe\n8dQ6kV9WYhS9Ql18urmU13IL2Vley/CUGGLCQwgNDiImIpg1hZXkFpS3nVMR4pK260tEhQVzUmIU\ng5Oi6RcXwd7KOkr2N9DcosyaPJDMxEhW76xgaHI0cb1Ce+Q1WiIwxhgvcDe3sKWkhjXOwHZ9UzP1\nTc2U1zayuXg/BWW1NLcoQQLxUWHUNTZT19RMZKiLKmfW2FFpsYxKjyM1LoKosGBaFCYM7M3AhCi2\nluwnPMRFSmz4MZ9vYWMExhjjBcGuIIYkRzMkObrT5Q3uZoqrGkiKCSc0OIiK2kb+8uEmKuuaOGd4\nEut2V/HltjJeXr6D+qaDB7tby1xBQnJMONeemsENUzK7/3V0+xaNMcYAEBbsIr1Pr7bHcb1CuWd6\ndtvj1nMtVJWqOjc1jW7czcoH64vYua+WEamxuFtaKCyvo7C8jsRo78zpZInAGGN8TESI7RVCbC/P\nJUivP21gjz6/TfJhjDEBzhKBMcYEOEsExhgT4CwRGGNMgLNEYIwxAc4SgTHGBDhLBMYYE+AsERhj\nTIA77uYaEpESYPtRrp4AlHZjON3JX2OzuI6Mv8YF/hubxXVkjjauAaqa2NmC4y4RHAsRye1q0iVf\n89fYLK4j469xgf/GZnEdGW/EZV1DxhgT4CwRGGNMgAu0RPC4rwM4BH+NzeI6Mv4aF/hvbBbXken2\nuAJqjMAYY8zBAq1FYIwxpgNLBMYYE+ACJhGIyPkislFENovIbB/GkS4iH4vIOhFZKyK/cMrvEZFd\nIrLauV3og9gKROQr5/lznbI+IvKBiGxy/vb2QVxD2u2X1SJSJSK3+mKficjTIlIsIl+3K+t0H4nH\nX53P3BoRObmH4/qDiGxwnvstEYlzyjNEpK7dfnush+Pq8n0Tkbuc/bVRRM7zVlyHiG1eu7gKRGS1\nU96T+6yr7wjvfc5U9YS/AS5gC5AJhAL5wHAfxZICnOzcjwa+AYYD9wC3+3g/FQAJHcp+D8x27s8G\nHvSD93IvMMAX+wyYCpwMfP1t+wi4EHgPEGAi8GUPx3UuEOzcf7BdXBnt6/lgf3X6vjn/B/lAGDDQ\n+Z919WRsHZb/CfiND/ZZV98RXvucBUqLYAKwWVW3qmoj8ApwsS8CUdU9qprn3K8G1gOpvojlMF0M\nzHXuzwUu8WEsAGcBW1T1aM8uPyaqugTY16G4q310MfCcenwBxIlISk/Fpar/UlW38/ALIM0bz32k\ncR3CxcArqtqgqtuAzXj+d3s8NhER4PvAy956/q4c4jvCa5+zQEkEqcDOdo8L8YMvXxHJAMYAXzpF\nNztNu6d90QUDKPAvEVkpIjc6ZUmquse5vxdI8kFc7c3gwH9OX+8z6Hof+dPn7jo8vxpbDRSRVSKy\nWESm+CCezt43f9pfU4AiVd3UrqzH91mH7wivfc4CJRH4HRGJAt4AblXVKuBR4CRgNLAHT7O0p52m\nqicDFwA/E5Gp7Reqpx3qs+ONRSQUmA685hT5wz47gK/3UWdE5G7ADbzoFO0B+qvqGOA24CURienB\nkPzufevEDznwB0eP77NOviPadPfnLFASwS4gvd3jNKfMJ0QkBM8b/KKqvgmgqkWq2qyqLcATeLFJ\n3BVV3eX8LQbecmIoam1mOn+Lezqudi4A8lS1CPxjnzm62kc+/9yJyCzgIuBK58sDp+ulzLm/Ek9f\n/OCeiukQ75vP9xeAiAQDlwHzWst6ep919h2BFz9ngZIIVgCDRGSg86tyBvCOLwJx+h6fAtar6v+2\nK2/fp3cp8HXHdb0cV6SIRLfexzPQ+DWe/TTTqTYTmN+TcXVwwK80X++zdrraR+8A1zhHdUwEKts1\n7b1ORM4H7gSmq2ptu/JEEXE59zOBQcDWHoyrq/ftHWCGiISJyEAnruU9FVc7ZwMbVLWwtaAn91lX\n3xF483PWE6Pg/nDDM7L+DZ5MfrcP4zgNT5NuDbDauV0IPA985ZS/A6T0cFyZeI7YyAfWtu4jIB5Y\nBGwCPgT6+Gi/RQJlQGy7sh7fZ3gS0R6gCU9f7PVd7SM8R3H8n/OZ+woY18NxbcbTd9z6OXvMqXu5\n8x6vBvKA7/ZwXF2+b8Ddzv7aCFzQ0++lU/4s8JMOdXtyn3X1HeG1z5lNMWGMMQEuULqGjDHGdMES\ngTHGBDhLBMYYE+AsERhjTICzRGCMMQHOEoEJWCLyufM3Q0R+1M3b/q/OnssYf2SHj5qAJyJn4JkN\n86IjWCdY/z2hW2fL96tqVHfEZ4y3WYvABCwR2e/cnQNMceaZ/6WIuMQzl/8KZ2K0Hzv1zxCRpSLy\nDrDOKXvbmaRvbetEfSIyB4hwtvdi++dyzv78g4h8LZ5rP/yg3bY/EZHXxXMNgRedM0yN8bpgXwdg\njB+YTbsWgfOFXqmq40UkDPhMRP7l1D0ZyFHPNMkA16nqPhGJAFaIyBuqOltEblbV0Z0812V4Jlsb\nBSQ46yxxlo0BsoHdwGfAqcCn3f9yjTmQtQiMOdi5eOZuWY1n+t94PHPLACxvlwQAbhGRfDzz/ae3\nq9eV04CX1TPpWhGwGBjfbtuF6pmMbTWei6EY43XWIjDmYAL8XFUXHlDoGUuo6fD4bGCSqtaKyCdA\n+DE8b0O7+83Y/6fpIdYiMAaq8VwSsNVC4CZnKmBEZLAzI2tHsUC5kwSG4rlMYKum1vU7WAr8wBmH\nSMRzuURfzLBpTBv7xWGMZ5bHZqeL51ngITzdMnnOgG0JnV+i833gJyKyHs9smV+0W/Y4sEZE8lT1\nynblbwGT8MzyqsCdqrrXSSTG+IQdPmqMMQHOuoaMMSbAWSIwxpgAZ4nAGGMCnCUCY4wJcJYIjDEm\nwFkiMMaYAGeJwBhjAtz/B0men7f5QIjkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load and preprocessing data\n",
    "x_train, y_train, x_test, y_test = preprocessing(*load_data())\n",
    "\n",
    "# # data augmentation\n",
    "# img_gen = ImageDataGenerator(\n",
    "#     width_shift_range  = 0.1,\n",
    "#     height_shift_range = 0.1,\n",
    "#     rotation_range     = 15,\n",
    "#     horizontal_flip    = True\n",
    "# )\n",
    "# img_gen.fit(x_train)\n",
    "\n",
    "\n",
    "# define model\n",
    "model_c100 = define_model()\n",
    "model_c100.summary()\n",
    "\n",
    "# fit model\n",
    "history_c100 = model_c100.fit(\n",
    "    x_train, y_train, epochs=200, \n",
    "    batch_size=64, \n",
    ")\n",
    "\n",
    "model_c100.save('model_c100.h5')\n",
    "\n",
    "# plot history\n",
    "plot(history_c100.history, title=\"CIFAR 100 CONV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 629,
     "status": "error",
     "timestamp": 1581031101959,
     "user": {
      "displayName": "Mingyang Zhao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBU_4wI4WvnINy6zSsai9F1QxIA_IipMaofL5MFhDOTeReb5ylbXw_ivpZDOI5wsqk98RaR0qRyDCUUfFSzhGgIoLmxFA3QaMTUnQmV2fdeQBHBrhw-TdI1KzVrHNVKHHnRzlJgdzEhX2VdgVfr1-swZoVvqC4I7mqiacBbscwI6t4kZKm7sgBHxVcqdsciEmYRvktvMX4ukT1WM9ixLs8_gj0ECnkcK0uTM6Vj8R6f7b-xupkFz6pkEaj3V1I07ma5u-j_hcwdb0b-9BGbiMFJMnbyKWpYWwlhUgze94OFW3MVlRo6DzlqzqNep58KPMUoof2hWUrPZEPfSCUhIX47CQ3vSXeCNtjzQf9D1gZV7LfAfq7wjp76UE38-8sfKdb4Bk0_vjjjaSPb8M9iw2jxpDnSzOKBTlztXA91x35R6ZkQo0Yxsfvr7F5e0lFnza7WJm8PzO-hE0kTFpHTwUCwwE3RVNGJd2pAzQzrHx4GtDB7Y74Iry6xo575jdrYgbO7pLBToS1wnywpK5KKT0nOxXTcPhBiUjzKEyFeVXVyYWYpyGpdbHbETi5STP59GeSpPXBnjgxNIQ2tYYkeRSnJMPE3JeygS3wvLjVcqVwZu809cnKmM1pC2M7TzuV2n1BX_9SAf64GVBRtyNtk35H7uwyZB1hrcggRT3na3OkXdIwJXjWbVOIOI9Q9MZRdidDFx9S-EF7i2xByOZeW9vT30s0K9yl2Tui328a989Nk8ycbDgo_GOq4cTHQqzQ=s64",
      "userId": "06904678479789034552"
     },
     "user_tz": 480
    },
    "id": "-fgA7qYmzAiT",
    "outputId": "43c06916-2525-4efe-eea3-9fbd07da401b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a8fb5d2e691a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_c100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CIFAR 100 CONV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history_c100' is not defined"
     ]
    }
   ],
   "source": [
    "# plot history\n",
    "plot(history_c100, title=\"CIFAR 100 CONV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13314,
     "status": "ok",
     "timestamp": 1581022354339,
     "user": {
      "displayName": "Mingyang Zhao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBU_4wI4WvnINy6zSsai9F1QxIA_IipMaofL5MFhDOTeReb5ylbXw_ivpZDOI5wsqk98RaR0qRyDCUUfFSzhGgIoLmxFA3QaMTUnQmV2fdeQBHBrhw-TdI1KzVrHNVKHHnRzlJgdzEhX2VdgVfr1-swZoVvqC4I7mqiacBbscwI6t4kZKm7sgBHxVcqdsciEmYRvktvMX4ukT1WM9ixLs8_gj0ECnkcK0uTM6Vj8R6f7b-xupkFz6pkEaj3V1I07ma5u-j_hcwdb0b-9BGbiMFJMnbyKWpYWwlhUgze94OFW3MVlRo6DzlqzqNep58KPMUoof2hWUrPZEPfSCUhIX47CQ3vSXeCNtjzQf9D1gZV7LfAfq7wjp76UE38-8sfKdb4Bk0_vjjjaSPb8M9iw2jxpDnSzOKBTlztXA91x35R6ZkQo0Yxsfvr7F5e0lFnza7WJm8PzO-hE0kTFpHTwUCwwE3RVNGJd2pAzQzrHx4GtDB7Y74Iry6xo575jdrYgbO7pLBToS1wnywpK5KKT0nOxXTcPhBiUjzKEyFeVXVyYWYpyGpdbHbETi5STP59GeSpPXBnjgxNIQ2tYYkeRSnJMPE3JeygS3wvLjVcqVwZu809cnKmM1pC2M7TzuV2n1BX_9SAf64GVBRtyNtk35H7uwyZB1hrcggRT3na3OkXdIwJXjWbVOIOI9Q9MZRdidDFx9S-EF7i2xByOZeW9vT30s0K9yl2Tui328a989Nk8ycbDgo_GOq4cTHQqzQ=s64",
      "userId": "06904678479789034552"
     },
     "user_tz": 480
    },
    "id": "plfuWMjzKKoq",
    "outputId": "e9915ff6-1a71-4a2e-ac25-1e4cd0f2e7d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.5699 - accuracy: 0.5670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5698779359817505, 0.56702]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c100.evaluate(x_train_c100, y_train_c100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nCb2KBQLHsc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-89c56a199ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 14 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "to_categorical([5,1,7,14],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMyTUF8yQ1pW/M1Nym3+McD",
   "collapsed_sections": [],
   "name": "lab-4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
