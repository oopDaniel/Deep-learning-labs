{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shapes, outputs):\n",
    "  \"\"\"Initializes weights of model according to shape.\n",
    "\n",
    "     Args:\n",
    "       shapes = [784, 300, 10]\n",
    "       outputs = [\"relu\", \"sigmoid\"]\n",
    "     returns:\n",
    "       model with uniform random weights [-1,+1], zero bias and output function\n",
    "       [\n",
    "        [random(784, 300), zeros(300), \"relu\"]\n",
    "        [random(300, 10), zeros(10), \"sigmoid\"] \n",
    "      ]\n",
    "  \"\"\"\n",
    "  models = []\n",
    "  for i in range(len(shapes)):\n",
    "    if i is 0:\n",
    "      continue\n",
    "\n",
    "    models.append([\n",
    "      np.random.uniform(-1, 1, (shapes[i - 1], shapes[i])),\n",
    "      np.zeros(shapes[i]).reshape(1, shapes[i]),\n",
    "      outputs[i - 1]\n",
    "    ])\n",
    "\n",
    "  return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  \"\"\"Computes relu of function.\"\"\"\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "  \"\"\"Computes sigmoid of function.\"\"\"\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu_derivative(z):\n",
    "  \"\"\"Computes derivative of relu of function.\"\"\"\n",
    "  z_copy = np.array(z, copy=True)\n",
    "  z_copy[z_copy <= 0] = 0\n",
    "  z_copy[z_copy > 0] = 1\n",
    "  return z_copy\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "  \"\"\"Computes derivative of sigmoid of function.\"\"\"\n",
    "  sig = sigmoid(z)\n",
    "  return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(z, funcName):\n",
    "  \"\"\"Activate the given input based on activation function name\n",
    "  \n",
    "     Args:\n",
    "       z: input tensor of shape (B, Ni)\n",
    "       funcName: string of supported activation function name\n",
    "     Returns:\n",
    "       tensor of shape (B, Ni) after applied activation function\n",
    "  \"\"\"\n",
    "  supported_funcs = {\n",
    "    \"relu\": relu,\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"linear\": lambda x: x\n",
    "  }\n",
    "\n",
    "  if funcName not in supported_funcs:\n",
    "    raise Exception(\"Unsupported function\")\n",
    "\n",
    "  return supported_funcs[funcName](z)\n",
    "  \n",
    "\n",
    "def forward(x, model):\n",
    "  \"\"\"Performs forward pass of training step.\n",
    "\n",
    "     Args:\n",
    "       x: input tensor of shape (B, Ni)\n",
    "       model: list of model weights (see initialize weights)\n",
    "     Returns:\n",
    "       List containing dictionary { \"y\": y, \"z\": z } for each layer of network.\n",
    "  \"\"\"     \n",
    "\n",
    "  # Add input as first layer, which will be consumed in backward\n",
    "  layer_output = [{\"z\": x, \"y\": x}]\n",
    "\n",
    "  for w, b, activation_func in model:\n",
    "    y_prev = layer_output[-1][\"y\"]\n",
    "    \n",
    "    z = np.dot(y_prev, w) + b\n",
    "    y = activate(z, activation_func)\n",
    "    \n",
    "    layer_output.append({\"z\": z, \"y\": y})\n",
    "\n",
    "  return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "  \"\"\"Predicts the output of a model.\n",
    "\n",
    "     Args:\n",
    "       x: input tensor of shape (B, Ni)\n",
    "       model: list of model weights (see initialize weights)\n",
    "     Returns:\n",
    "       Prediction of model, with the same shape as the labeled data (B, No).\n",
    "  \"\"\"\n",
    "  fwd = forward(x, model)\n",
    "  return fwd[-1][\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, p):\n",
    "  \"\"\"Computes Mean-Square Error between y and p.\n",
    "     Args:\n",
    "       y: labeled data of size (B, No) \n",
    "       p: predicted label of size (B, No)\n",
    "     Returns:\n",
    "       MSE of y-p\n",
    "  \"\"\"\n",
    "  return np.mean(np.square(y - p))\n",
    "\n",
    "def mse_derivative(y, p):\n",
    "  \"\"\"Computes derivative of Mean-Square Error between y and p.\n",
    "     Args:\n",
    "       y: labeled data of size (B, No) \n",
    "       p: predicted label of size (B, No)\n",
    "     Returns:\n",
    "       derivative of MSE = y-p\n",
    "  \"\"\"\n",
    "  return p - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-96-57055c5f3436>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-96-57055c5f3436>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    return p - y, / np.multiply(p, 1 - p)\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def binary_crossentropy(y, p):\n",
    "  \"\"\"Computes binary crossentropy between y and p.\n",
    "     Args:\n",
    "       y: labeled data of size (B, No) \n",
    "       p: predicted label of size (B, No)\n",
    "     Returns:\n",
    "       BCE of (y, p) = mean(sum(y log(p) + (1-y) log(1-p))) \n",
    "  \"\"\"\n",
    "  return - np.mean(\n",
    "        np.multiply(y, np.log(p)) + np.multiply((1 - y), np.log(1 - p)))\n",
    "\n",
    "def binary_crossentropy_derivative(y, p):\n",
    "  \"\"\"Computes derivative of binary crossentropy between y and p.\n",
    "     Args:\n",
    "       y: labeled data of size (B, No) \n",
    "       p: predicted label of size (B, No)\n",
    "     Returns:\n",
    "       derivative of BCE of (y, p) = -[y / p - (1 - y) / (1 - p)]\n",
    "  \"\"\"\n",
    "  return p - y, / np.multiply(p, 1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(y, p, funcName, is_last_layer_error=False):\n",
    "  \"\"\"Calculate lost based on cost function name.\n",
    "  \n",
    "     Args:\n",
    "       y: input tensor of shape (B, Ni)\n",
    "       p: label vector of shape (B, 1)\n",
    "       funcName: string of supported lost function name\n",
    "     Returns:\n",
    "       tensor of shape (B, Ni) after applied cost function\n",
    "  \"\"\"\n",
    "  supported_cost_funcs = {\n",
    "    \"mse\": mse,\n",
    "    \"binary_crossentropy\": binary_crossentropy,\n",
    "  }\n",
    "    \n",
    "  supported_cost_funcs_derivative = {\n",
    "    \"mse\": mse_derivative,\n",
    "    \"binary_crossentropy\": binary_crossentropy_derivative,\n",
    "  }\n",
    "\n",
    "  # Should also appear in supported_cost_funcs_derivative \n",
    "  if funcName not in supported_cost_funcs:\n",
    "    raise Exception(\"Unsupported function\")\n",
    "\n",
    "  func = supported_cost_funcs_derivative if is_last_layer_error else supported_cost_funcs\n",
    "\n",
    "  return func[funcName](y, p)\n",
    "\n",
    "def get_activation_derivative(z, funcName):\n",
    "  \"\"\"Computes derivative of activation function for computing dZ.\n",
    "     Args:\n",
    "       z: input tensor of shape (B, Ni)\n",
    "       funcName: string of supported activation function name\n",
    "     Returns:\n",
    "       derivative of activation function\n",
    "  \"\"\"\n",
    "  supported_funcs = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "    \"linear\": lambda _: 1\n",
    "  }\n",
    "\n",
    "  if funcName not in supported_funcs:\n",
    "    raise Exception(\"Unsupported function\")\n",
    "\n",
    "  return supported_funcs[funcName](z)\n",
    "\n",
    "def backward(y, x, models, loss):\n",
    "  \"\"\"Computes backward step of training.\n",
    "     Args:\n",
    "       y: labeled data of size (B, No) \n",
    "       x: input tensor of shape (B, Ni)\n",
    "       model: list of model weights (see initialize weights)\n",
    "       loss: one of (\"mse\", \"binary_crossentropy\")\n",
    "     Returns:\n",
    "       tuple with loss evaluation of (y, predict(x)) and list of dictionary\n",
    "       containing { \"dw\": dw, \"db\": db } for each layer of network. Remember\n",
    "       that shape of dw for each layer should be equal to shape of weight for\n",
    "       the same layer.\n",
    "  \"\"\"\n",
    "  y_predicted = x[-1][\"y\"]\n",
    "  y = y.reshape(y_predicted.shape)\n",
    "  b = y.shape[0] # number of training data\n",
    "\n",
    "  cost = get_cost(y, y_predicted, loss)\n",
    "  weights = []\n",
    "\n",
    "  dY_prev = get_cost(y, y_predicted, loss, is_last_layer_error=True)\n",
    "\n",
    "  for i, model in reversed(list(enumerate(models))):\n",
    "    dY_curr = dY_prev\n",
    "\n",
    "    Y_prev = x[i][\"y\"]\n",
    "    Z_curr = x[i + 1][\"z\"]\n",
    "    W_curr, _, activation_func = model\n",
    "    \n",
    "    dZ_curr = get_activation_derivative(Z_curr, activation_func) * dY_curr\n",
    "    dW_curr = np.dot(Y_prev.T, dZ_curr) / b\n",
    "    db_curr = np.sum(dZ_curr, axis=0, keepdims=True) / b\n",
    "    dY_prev = np.dot(dZ_curr, W_curr.T)\n",
    "\n",
    "    weights.insert(0, {\"dw\": dW_curr, \"db\": db_curr})\n",
    "\n",
    "  return (cost, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(weights, dweights, alpha):\n",
    "  \"\"\"Gradient descent for weights and biases.\"\"\"\n",
    "  for i in range(len(weights)):\n",
    "    weights[i][0] += - alpha * dweights[i][\"dw\"]\n",
    "    weights[i][1] += - alpha * dweights[i][\"db\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
